<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jiaqi&#39;s Blog</title>
  
  
  <link href="https://luuvy757.github.io/atom.xml" rel="self"/>
  
  <link href="https://luuvy757.github.io/"/>
  <updated>2023-02-23T02:05:09.254Z</updated>
  <id>https://luuvy757.github.io/</id>
  
  <author>
    <name>Jiaqi Shao</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Reflection of Research</title>
    <link href="https://luuvy757.github.io/2023/02/23/Reflection-of-Research/"/>
    <id>https://luuvy757.github.io/2023/02/23/Reflection-of-Research/</id>
    <published>2023-02-23T01:15:41.000Z</published>
    <updated>2023-02-23T02:05:09.254Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Important-points-of-research"><a href="#Important-points-of-research" class="headerlink" title="Important points of research"></a>Important points of research</h1><ul><li>Make a <strong>clear assumption</strong> is non-trivial, and it will pave the way for your research. Any missed assumption will lead to a weak research.</li><li><strong>Technicial details</strong> should be convining and convincing! Any flawed steps will lead to a wrong conclusion, or even disturb the whole research!</li><li>The experiments should have a <strong>clear goal</strong>, and the results should <strong>convince your work</strong> or give insights on further directions.</li><li>The experiments should consider <strong>general cases</strong>, like using more metrics, more datasets, comparing with more baselines, etc.</li></ul><h1 id="About-technical-writing"><a href="#About-technical-writing" class="headerlink" title="About technical writing"></a>About technical writing</h1><p>The writing of the whole paper can help you understand/remember things, and test your ideas.<br>It is a silent communication with your readers:</p><p>Unlike normal communication, he is not interactive, but you have to think about what kind of process the whole communication should be at the beginning, and to figure out who the readers are and what they need.<br>You should bring in the reader‚Äôs identity, think about what the reader may be confused, and then adjust some points appropriately.</p><ul><li><strong>It is necessary to think for readers</strong>: think about how audiences will understand these, and how can you convince them to believe your research.</li><li>The logic of the writing should be clear and easy to understand. <ul><li>What are the research questions? </li><li>Why these questions are important? </li><li>How to solve these questions? </li><li>Do you completely sovle these questions? </li><li>Does your experiments/theorems/proofs are correct to convince your readers?</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Important-points-of-research&quot;&gt;&lt;a href=&quot;#Important-points-of-research&quot; class=&quot;headerlink&quot; title=&quot;Important points of research&quot;&gt;&lt;/a&gt;Im</summary>
      
    
    
    
    <category term="research" scheme="https://luuvy757.github.io/categories/research/"/>
    
    
    <category term="reflection" scheme="https://luuvy757.github.io/tags/reflection/"/>
    
  </entry>
  
  <entry>
    <title>FL Survey</title>
    <link href="https://luuvy757.github.io/2023/01/20/FL-Survey/"/>
    <id>https://luuvy757.github.io/2023/01/20/FL-Survey/</id>
    <published>2023-01-20T02:17:09.000Z</published>
    <updated>2023-02-23T02:04:04.455Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h1><h2 id="Federated-Machine-Learning-Concept-and-Applications-Paper-‚úîÔ∏è"><a href="#Federated-Machine-Learning-Concept-and-Applications-Paper-‚úîÔ∏è" class="headerlink" title="Federated Machine Learning: Concept and Applications [Paper] ‚úîÔ∏è"></a><strong>Federated Machine Learning: Concept and Applications</strong> <a href="https://dl.acm.org/citation.cfm?id=3298981" title="\[Paper]">[Paper]</a> ‚úîÔ∏è</h2><h2 id="Federated-Learning-Challenges-Methods-and-Future-Directions-Paper-‚úîÔ∏è"><a href="#Federated-Learning-Challenges-Methods-and-Future-Directions-Paper-‚úîÔ∏è" class="headerlink" title="Federated Learning: Challenges, Methods, and Future Directions [Paper] ‚úîÔ∏è"></a><strong>Federated Learning: Challenges, Methods, and Future Directions</strong> <a href="https://arxiv.org/abs/1908.07873" title="\[Paper]">[Paper]</a> ‚úîÔ∏è</h2><ol><li><p><strong>Challenge 1: Expensive Communication.</strong> (i) reducing the total number of communication rounds, or (ii) reducing the <strong>size of transmitted messages</strong> at each round.</p><ol><li><p>Local Updating: reduce the total number of communication rounds</p><img src="/2023/01/20/FL-Survey/image_GMlwXt6aaQ.png" class=""></li><li>Compression Scheme: reduce the size of messages communicated at each round. (sparsification, subsampling, and quantization)</li><li><p>Decentralized Training</p><img src="/2023/01/20/FL-Survey/image_PeWLFhmo-E.png" class=""></li></ol></li><li><p><strong>Challenge 2: Systems Heterogeneity</strong>.variability in hardware (CPU, memory), network connectivity (3G, 4G, 5G, wifi), and power (battery level).(i) anticipate a low amount of participation, (ii) tolerate heterogeneous hardware, and (iii) be robust to dropped devices in the network.</p><ol><li><p>Asynchronous communication: mitigate <em>stragglers</em> in heterogeneous environments</p><img src="/2023/01/20/FL-Survey/image_LjfrgCZebd.png" class=""></li><li>Active device sampling: sampling policies based on systems resources, incentive mechanisms to encourage devices with higher-quality data.</li><li>Fault tolerance.</li></ol></li><li><p><strong>Challenge 3: Statistical Heterogeneity</strong>: non-iid data distribution, varying the number of data samples</p><ol><li><p>Modeling Heterogeneous Data</p><img src="/2023/01/20/FL-Survey/image_ZBNjxqb3K2.png" class=""></li><li>Convergence Guarantees for Non-IID Data</li></ol></li><li>Challenge 4: Privacy Concerns.<h2 id="Advances-and-Open-Problems-in-Federated-Learning-Paper"><a href="#Advances-and-Open-Problems-in-Federated-Learning-Paper" class="headerlink" title="Advances and Open Problems in Federated Learning [Paper]"></a><strong>Advances and Open Problems in Federated Learning</strong> <a href="https://arxiv.org/abs/1912.04977" title="\[Paper]">[Paper]</a></h2><h2 id="Federated-Learning-White-Paper-V1-0-Paper-‚úîÔ∏è"><a href="#Federated-Learning-White-Paper-V1-0-Paper-‚úîÔ∏è" class="headerlink" title="Federated Learning White Paper V1.0 [Paper] ‚úîÔ∏è"></a>Federated Learning White Paper V1.0 <a href="https://www.fedai.org/static/flwp-en.pdf" title="\[Paper]">[Paper]</a> ‚úîÔ∏è</h2><h2 id="Federated-Learning-Systems-Vision-Hype-and-Reality-for-Data-Privacy-and-Protection-Paper-‚úîÔ∏è"><a href="#Federated-Learning-Systems-Vision-Hype-and-Reality-for-Data-Privacy-and-Protection-Paper-‚úîÔ∏è" class="headerlink" title="Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection [Paper] ‚úîÔ∏è"></a>Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection <a href="https://arxiv.org/abs/1907.09693" title="\[Paper]">[Paper]</a> ‚úîÔ∏è</h2></li></ol><ul><li>Definition: Federated systems have different emphasis on collaboration and constraints. (1) FDBSs focus on the <strong>management</strong> of distributed data and (2) FCs [federated cloud] focus on the <strong>scheduling</strong> of the resources, (3) FLSs [Federated Learning System] care more about the secure computation among multiple parties. FLSs induce new challenges such as the algorithm designs of the distributed training and the data protection under the privacy restrictions.<ul><li>Parties:&#x20;<ul><li>the hardware capacity [e.g. the computation power and storage]&#x20;</li><li>the scale and stability: mobile devices are more scalable. Also, the stability of the cross-silo is better than the cross-device setting.&#x20;</li><li>the data distributions [e.g. non-iid]</li></ul></li><li>Manager: cross-device ‚Äî a powerful central server; cross-silo ‚Äî selected organization; (blockchain based)</li></ul></li><li>Evaluation: (1) model performance, (2) system security, (3) system efficiency, and (4) system robustness.<img src="/2023/01/20/FL-Survey/image_Cr8FuyVvkG.png" class=""></li></ul><img src="/2023/01/20/FL-Survey/image_aMFRwwuJpO.png" class="" title="Taxonomy of federated learning systems"><!-- ![Taxonomy of federated learning systems](image/image_aMFRwwuJpO.png "Taxonomy of federated learning systems") --><img src="/2023/01/20/FL-Survey/image_Yj5zT8RULN.png" class=""><h2 id="Federated-Learning-in-Mobile-Edge-Networks-A-Comprehensive-Survey-Paper-‚úîÔ∏è"><a href="#Federated-Learning-in-Mobile-Edge-Networks-A-Comprehensive-Survey-Paper-‚úîÔ∏è" class="headerlink" title="Federated Learning in Mobile Edge Networks: A Comprehensive Survey [Paper] ‚úîÔ∏è"></a>Federated Learning in Mobile Edge Networks: A Comprehensive Survey <a href="https://arxiv.org/abs/1909.11875" title="\[Paper]">[Paper]</a> ‚úîÔ∏è</h2><img src="/2023/01/20/FL-Survey/image_20SuT40krc.png" class=""><ul><li><p><strong>Communication Cost</strong></p><img src="/2023/01/20/FL-Survey/image_M0xWkQEWNe.png" class=""><ul><li>Edge and end computation (decrease the number of communication rounds)<ul><li>increasing <strong>parallelism</strong>: <strong>more participants</strong> are selected to participate in each round of training</li><li>increasing computation per participant: each participant performs more <em>local updates</em> before communication for global aggregation.<br><img src="https://pdf.cdn.readpaper.com/parsed/fetch_target/739e63e50300d71de36e2952d6cc768f_9_Figure_6.png" alt="Approaches to increase computation at edge and end devices include (a) Increased computation at end devices, e.g., more passes over dataset before communication(b) Two-stream training with global model as a reference and (c) Intermediate edge server aggregation." title="Approaches to increase computation at edge and end devices include (a) Increased computation at end devices, e.g., more passes over dataset before communication(b) Two-stream training with global model as a reference and (c) Intermediate edge server aggregation."></li></ul></li><li>Model compression<ul><li>structured updates: restrict participant updates to have a pre-specific structure. [<em>low rank</em> \<only optimized matrix needs to be sent> and <em>random mask</em> \<only non-zero entries needs to be sent>]</li><li>sketched updates:  encoding the update in a compressed form before communication [subsampling approach, probabilistic qantization approach]<br><img src="image/image_Tu2xb7Ir1n.png" alt="The compression techniques considered are summarized above by the diagram from authors in \[99\]. (i) Federated dropout to reduce size of model (ii) Lossy compression of model (iii) Decompression for training (iv) Compression of participant updates (v) Decompression (vi) Global aggregation" title="The compression techniques considered are summarized above by the diagram from authors in \[99]. (i) Federated dropout to reduce size of model (ii) Lossy compression of model (iii) Decompression for training (iv) Compression of participant updates (v) Decompression (vi) Global aggregation"></li></ul></li><li>Important-based updating<ul><li>edge Stochastic Gradient Descent (eSGD) algorithm: selects only a small fraction of important gradients to be communicated to the FL server for parameter update during each communication round.</li></ul></li></ul></li><li><p><strong>Resource Allocation</strong></p><img src="/2023/01/20/FL-Survey/image_g4W43CyUhk.png" class=""><ul><li><p>Participant Selection: less degree of non-iid, computation capabilities, higher loss (fairness allocation: <em>variance</em> of performance of an FL model across participants)</p><img src="/2023/01/20/FL-Survey/image_yaGoaxcm3u.png" class=""></li><li>Joint Radio and Computation Resource Management: orthogonal access schemes, communication latency increases in direct proportion with the number of participants; for <strong>multiaccess schemes</strong>, latency is independent of the number of participants.</li><li>Adaptive Aggragation</li><li>Incentive Mechanism</li></ul></li><li><p><strong>Privacy and Security</strong></p><ul><li><p>Privacy issues</p><ul><li>Information exploiting attacks in machine learning:  inferring information from a trained model</li><li>Differential privacy-based protection solution</li><li><p>Collaborative training solutions:</p><ul><li><p>select gradients/parameters to update</p><img src="/2023/01/20/FL-Survey/image_i_UTTnvE8w.png" class=""></li><li><p>powerful attack, based on GANs, which allows a <strong>malicious participant</strong> to infer sensitive information from a <strong>victim participant</strong> even with just a part of <strong>shared parameters</strong> from the victim. ‚Üê secret sharing scheme (rely on a trusted third party to generate signature key pairs)</p><img src="/2023/01/20/FL-Survey/image_NVRS7KdmeO.png" class=""></li><li>Collaborative training model: cooperate to train a federated GANs model. (fed GANs can generate artificial data replacing real data for the honest participants). Federated generative model <strong>output artificial data</strong> not belong to any real participants, but comes from the common cross-user data distribution. ‚Üí training instability &amp; lower performance</li></ul></li><li>Encryption-based Solutions</li></ul></li><li><p>Security issues</p><img src="/2023/01/20/FL-Survey/image_mQn5SLRTBs.png" class=""><ul><li>Data Poisoning Attacks: <strong>Dirty-label data</strong> poisoning attacks, a sybil-based data poisoning attack by creating multiple malicious participants</li><li>Model Poisoning Attacks: poison the global model that it sends to the server for aggregation. ‚Üê check whether the shared model can help to improve the global model‚Äôs performance or not. &amp; an updated model from a participant is <strong>too different</strong> from the others</li><li><p>Free-Riding Attacks: a participant wants to benefit from the global model without contributing to the learning process. ‚Üê Blockchain based, rewards</p><img src="/2023/01/20/FL-Survey/image_LD35zj88bP.png" class=""></li></ul></li></ul></li><li><p>FL for MEN (applications of FL)</p><img src="/2023/01/20/FL-Survey/image_SaD4vOtyW5.png" class=""><h2 id="Federated-Learning-for-Wireless-Communications-Motivation-Opportunities-and-Challenges-Paper-‚úîÔ∏è"><a href="#Federated-Learning-for-Wireless-Communications-Motivation-Opportunities-and-Challenges-Paper-‚úîÔ∏è" class="headerlink" title="Federated Learning for Wireless Communications: Motivation, Opportunities and Challenges [Paper] ‚úîÔ∏è"></a>Federated Learning for Wireless Communications: Motivation, Opportunities and Challenges <a href="https://arxiv.org/abs/1908.06847" title="\[Paper]">[Paper]</a> ‚úîÔ∏è</h2></li></ul><img src="/2023/01/20/FL-Survey/image_t2GDn4qdLg.png" class=""><p>Applications of FL for 5G [difficult to understand‚Ä¶üò¢]</p><ul><li>Edge Computing and Caching</li><li>Spectrum Management</li><li><p>5G Core Network</p><h2 id="A-Review-of-Applications-in-Federated-Learning-Paper-‚úîÔ∏è"><a href="#A-Review-of-Applications-in-Federated-Learning-Paper-‚úîÔ∏è" class="headerlink" title="A Review of Applications in Federated Learning [Paper] ‚úîÔ∏è"></a>A Review of Applications in Federated Learning <a href="https://www.sciencedirect.com/science/article/abs/pii/S0360835220305532" title="\[Paper]">[Paper]</a> ‚úîÔ∏è</h2><img src="/2023/01/20/FL-Survey/image_6XTzYhzv48.png" class=""><img src="/2023/01/20/FL-Survey/image_c6s1iSM5Tp.png" class=""><img src="/2023/01/20/FL-Survey/image_bhdT4NxPA0.png" class=""><h2 id="Towards-Efficient-Synchronous-Federated-Training-A-Survey-on-System-Optimization-Strategies-Paper-‚úîÔ∏è"><a href="#Towards-Efficient-Synchronous-Federated-Training-A-Survey-on-System-Optimization-Strategies-Paper-‚úîÔ∏è" class="headerlink" title="Towards Efficient Synchronous Federated Training: A Survey on System Optimization Strategies [Paper] ‚úîÔ∏è"></a>Towards Efficient Synchronous Federated Training: A Survey on System Optimization Strategies <a href="https://ieeexplore.ieee.org/document/9780218" title="\[Paper]">[Paper]</a> ‚úîÔ∏è</h2></li></ul><p><strong><em>Time-to-accuracy</em></strong>, which is the wall clock time taken to train a model until it reaches the target accuracy [evaluation metric for efficiency]</p><p><strong><em>Challenges</em></strong></p><ol><li><strong>the lack of information for optimization:</strong> the information needed for optimally configuring the system is usually outdated or simply unavailable due to privacy constraints and scaling issues;</li><li><strong>the tradeoff between statistical and system utility:</strong> statistical utility (the number of iterations taken to reach a plausible target accuracy) and system utility (the duration of an iteration), the two determining factor for <em>time-to-accuracy</em>, are usually at odds in FL;&#x20;<ol><li>Compare <strong><em>baseline</em></strong>, <strong><em>non-iid</em></strong> [latent Dirichlet allocation], <strong><em>straggler</em></strong> [computation speeds follow the Zipf‚Äôs distribution]: \<statistical utility> <strong><em>baseline vs. non-iid</em></strong>; \<system utility > <strong><em>baseline vs. straggler</em></strong></li></ol></li><li><strong>client heterogeneity</strong>: clients cannot be treated uniformly due to the intrinsic differences in terms of resources, data, and states;&#x20;<ol><li>Resource Heterogeneity: different capabilities in computation (CPU/GPU/NPU memory, storage) &amp; communication (connectivity, bandwidth) &amp; power (battery level, lifespan) ‚Äî&gt; system utility</li><li>Data Heterogeneity: nonIID (sample quantity &amp; label partition {distribution of data labels}) ‚Äî&gt; statistical utility</li><li>State Heterogeneity: temporal distribution (screen locking, battery charging) ‚Äî&gt; statistical utility</li></ol></li><li><strong>a large configuration space</strong>: the operational dimensions for system developers are too many to explore within an acceptable time.&#x20;<br><strong><em>Optimizing the time-to-accuracy performance in FL: a layered approach</em></strong> that categorizes them by the training phases in which they take effect:&#x20;<img src="/2023/01/20/FL-Survey/image_lAlPoDDxyP.png" class=""></li></ol><img src="/2023/01/20/FL-Survey/image_KyYB5vNNZi.png" class=""><ol><li><strong>Selection</strong>: the server chooses clients for participation, there are mainly two lines of optimization efforts:&#x20;<ol><li><strong>prioritizing</strong> clients either with high statistical utility or system utility<ol><li>statistical utility oriented: clients with lower degree of non-iid [normalized Euclidean distance (<strong>CSFedAvg</strong>), Reinforced Learning (<strong>FAVOR</strong>)]</li><li>system utility oriented: In synchronous training, clients with the lowest system utility bottleneck the speed of a federation round. Bound the time usage: set a deadline for randomly selected clients‚Äô to report updates and ignoring any update submitted after the deadline. To avoid waste of computing resources, <strong>FedCS</strong> takes a step further by proactively selecting a set of clients whose participation is not likely to <em>miss the deadline</em> according to latency estimation results.</li></ol></li><li>explicitly considering both utilities and developing a more informed solution in response to client dynamics in practice.<ol><li>Coarse-Grained. TiFL divides clients into <strong>different tiers</strong> based on the <em>observed runtime performance</em>, and at each round only selects clients from the same tier for mitigating the waste of resources due to idle waiting for stragglers. To reduce the average iteration span, it also limits the number of times a (slow) tier can be selected. On top of that, the <em>statistical utility</em> is respected by prioritizing tiers with lower testing accuracy whenever there is more than one electable tier.</li><li>Fine-Grained. Oort associates each client with a continuous <strong>score</strong> and prioritizes those clients with higher scores. The score is meant to be a principled measurement of both the statistical utility (determined by the training loss) and the system utility (estimated from historical response latency).&#x20;</li></ol></li></ol></li><li><strong>Configuration</strong>: the server sends the global model to the selected clients with auxiliary configuration information (e.g., the number of local epochs or the reporting deadline), and clients perform local training, four lines of work:&#x20;<ol><li>&#x20;The first two lines advocate mitigating the communication cost by <em>reducing the model size</em> and decreasing the synchronization frequency <ol><li><strong>Reducing the model size</strong><ol><li><strong>Quantization</strong>. Quantization converts each scalar in a model update to its low-bit representation which takes up less space. <strong>[error feedback]</strong>: tackling the precision loss brought by quantization, the basic idea is to accumulate the previous quantization errors and compensate for them in the current round.</li><li><strong>Sketching</strong>. Existing quantization approaches assume the input values follow a certain distribution (e.g., uniform or bell-shaped), which may not always be the case in model updates. To be more general, some researchers introduce sketching methods where some memory-saving data structures are used to approximate the exact distribution of model update values in a single processing pass over the values.&#x20;</li><li><strong>Sparsification</strong>. allows each client to transmit only a sparse subset of its model updates, while the rest are accumulated and incorporated into future training. (compatible with cryptographic techniques?)</li></ol></li><li>Decreasing the synchronization frequency <ol><li>Client-Level.</li><li>Layer-Level. TWAFL DNN update shallow layers more frequently than deep ones as they are more responsible for the overall quality of the global model.</li><li>Parameter-Level. consider whether to synchronize for each round at the level of individual parameters. Noticing that each parameter usually evolves <em>in a transient-then-stable manner</em>, i.e., it first varies drastically and then settles down around a certain value with slight oscillation, APF proposes to stop synchronizing those parameters whose evolution has reached a stationary phase.</li></ol></li></ol></li><li>the last two lines minimize the computational overhead by <em>accelerating the training speed</em> in each round and reducing the number of training rounds<ol><li>Accelerating the training speed<ol><li>Load Balancing: consider system heterogeneous ‚Äî&gt; balance the <em>amount</em> of training data across clients &amp; data heterogeneity ‚Äî&gt; slow client with <em>important training data</em>; Computational Load: varying the number of optimization steps (FedProx) or complexity of local models (HeteroFL: clients with fewer capabilities train smaller sub-models); Communication load: maximize learning efficiency by optimal data batch size and uplink/downlink frame time slots.</li></ol></li><li>Reducing the number of training rounds (convergence speedup)<ol><li>Optimizer State Synchronization. If the clients‚Äô momenta are <em>separately</em> updated, they may <em>deviate</em> from each other due to non-IID ‚Äî&gt; clients‚Äô optimizer states are synchronized by the server periodically.</li><li>Client Bias Reduction. Data heterogeneity ‚Äî&gt; ‚Äúclient drift‚Äù: clients‚Äô model updates can be biased towards the minima of local objectives ‚Äî&gt; hinders the convergence of the global model. To reduce the variance across clients (1) regularize local objective functions for minimizing the drift (2) control variates borrowed from the convex optimization literature (SCAFFOLD, persistent client states); (3) Posterior averaging (FedPA, stateless clients) formulates the optimization as a posterior inference. Compared to traditional federated optimization, posterior inference can benefit from an increased amount of local computation without risking stagnating at inferior optima.&#x20;</li></ol></li></ol></li></ol></li><li><strong>Reporting</strong>: (aggregation)  two related optimizations:&#x20;<ol><li>reducing the aggregation latency by adopting <em>hierarchical methods</em> and developing <em>lightweight</em> privacy preserving methods&#x20;<ol><li>Hierarchical methods: changing the aggregation rules, the hierarchical designs focus on establishing the convergence to a single global model.  Clustered FL where clients are assigned to different groups and aggregation takes place within a group ‚Äî&gt; personalized models for each group of clients.</li><li><strong>Lightweight privacy preserving methods</strong> model aggregation with cryptographic techniques ‚Äî&gt; extra computation and communication overhead. (secure multiparty computation (SMPC)., Homomorphic Encryption)</li></ol></li><li>improving the long-term convergence rate with <strong>adaptive optimizers</strong> on the <strong>server.</strong>&#x20;<ol><li>In FL, originally no optimizer at the server ‚Äî&gt; generalize the existing aggregation algorithm. At each round, instead of collecting local model weights, the server instead collects their changes and treats these changes as the ‚Äúpseudo-gradient‚Äù for the <strong>server</strong>, which the server can use to update the global model with <strong>adaptive optimizers</strong>.<img src="/2023/01/20/FL-Survey/image_HU9QW1oSPU.png" class=""><strong>Training Datasets</strong>:&#x20;</li></ol></li></ol></li></ol><p>(1) Synthetic: derived from conventional ML datasets (e.g.,CIFAR, MNIST, and Fashion-MNIST). To <em>synthesize</em> the non-IID nature as in real FL scenarios, the data partitions in these datasets are typically formed by restricting the number of data classes each client has (e.g., partitioning by shard-based methods or latent Dirichlet allocation (LDA))</p><p>(2) Realistic: LEAF, FedScale, OARF</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Survey&quot;&gt;&lt;a href=&quot;#Survey&quot; class=&quot;headerlink&quot; title=&quot;Survey&quot;&gt;&lt;/a&gt;Survey&lt;/h1&gt;&lt;h2 id=&quot;Federated-Machine-Learning-Concept-and-Applicatio</summary>
      
    
    
    
    <category term="research" scheme="https://luuvy757.github.io/categories/research/"/>
    
    
    <category term="FL" scheme="https://luuvy757.github.io/tags/FL/"/>
    
  </entry>
  
  <entry>
    <title>technical writing</title>
    <link href="https://luuvy757.github.io/2022/09/15/technical-writing/"/>
    <id>https://luuvy757.github.io/2022/09/15/technical-writing/</id>
    <published>2022-09-15T12:08:58.000Z</published>
    <updated>2023-02-23T02:04:34.177Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Technical-Writing"><a href="#Technical-Writing" class="headerlink" title="Technical Writing"></a>Technical Writing</h1><p>Prepared by <a href="http://jianwei.ie.cuhk.edu.hk/">Jianwei Huang</a><br>Current version: Feb. 11, 2019<br>Department of Information Engineering, The Chinese University of Hong Kong<br>This is an evolving summary of various writing tips that I have complied over the years. Some of<br>them heavily reflect my own preferences. Others follow widely accepted practices, of which I<br>provide external links for support or more detailed explanations. If you have any questions or<br>suggestions, please feel free to contact me at jwhuang [at] ie.cuhk.edu.hk.</p><h2 id="Table-of-contents"><a href="#Table-of-contents" class="headerlink" title="Table of contents"></a>Table of contents</h2><ol><li>Excellent Writing Tips on the Internet</li><li>How-Tos</li></ol><ul><li>How to organize a paper</li><li>How to organize the list of contributions</li><li>How to organize the literature review</li><li>How to organize a section with many different concepts</li><li>How to organize core mathematical results</li><li>How to use key questions to emphasize key messages</li><li>How to organize the simulation results</li><li>How to present figures</li><li>How to present assumptions</li><li>How to define mathematical notations</li><li>How to use latexdiff to ‚Äútrack changes‚Äù in LaTex</li></ul><ol><li>Other Tips<h2 id="Excellent-Writing-Tips-on-the-Internet"><a href="#Excellent-Writing-Tips-on-the-Internet" class="headerlink" title="Excellent Writing Tips on the Internet"></a>Excellent Writing Tips on the Internet</h2>There are many excellent writing tips from the Internet. Here I just list a few.</li></ol><ul><li><a href="http://tex.loria.fr/typographie/mathwriting.pdf">‚ÄúMathematical Writing‚Äù</a> by Donald E. Knuth,<br>Tracy Larrabee, and Paul M. Roberts (class notes of 118 pages)</li><li><a href="http://web.cs.ucdavis.edu/~amenta/w10/writingman.pdf">‚ÄúA Guide to Writing Mathematics‚Äù</a> by<br>Dr. Kevin P. Lee (17 pages)</li><li><a href="https://cseweb.ucsd.edu/~swanson/papers/science-ofwriting.pdf">‚ÄúThe Science of Scientific Writing‚Äù</a> by by George D. Gopen and Judith A. Swan (16 pages)</li><li><a href="https://terrytao.wordpress.com/advice-onwriting-papers/">Terence Tao‚Äôs summary of mathematical writing tips</a><h2 id="How-Tos"><a href="#How-Tos" class="headerlink" title="How-Tos"></a>How-Tos</h2><h3 id="How-to-organize-a-paper"><a href="#How-to-organize-a-paper" class="headerlink" title="How to organize a paper"></a>How to organize a paper</h3>Writing a paper is similar as telling a story, and here are some general tips to make the story easy to<br>follow:<br>(1) The paper level:<br>Choose section titles to reflect a clear road-map of the paper. Very often we will have ‚ÄúIntroduction<br>-&gt; Literature Review (sometimes can be a subsection of the Introduction Section, especially for a<br>short conference paper) -&gt; System Model -&gt; ‚Ä¶ (middle section titles can vary significantly depend<br>on the problem we solve) -&gt; Simulation Results (or Numerical Results) -&gt; Conclusion and Future<br>Work‚Äù.<br>(2) The section level:<br>(2.a) Overview paragraph: For each section, start with a short overview paragraph that explains the<br>purpose of the section, how the rest of the section will be organized, and a very brief summary of<br>the key results (especially when the section is long or has several subsections). The overview<br>paragraph needs to hook the readers and encourage them to read on, knowing what to expect. This<br>is a key principle of technical writing: always give clear guidance to the readers regarding why he<br>needs to read on, and do not leave him wondering about the purpose of your writing.<br>(2.b) Organize the rest of the paragraphs within a section:</li><li>Make sure that the paragraphs in a section (and sentences in a paragraph) are connected with a<br>clear logic. There are many ways of organizing the logic: sequence, description, cause and effect,<br>compare and contrast, and problem and solution. Think carefully about what structure to use for<br>each section/paragraph, and use proper transition words to make the structure clear to the readers.<br>If necessary, conclude the key message again at the end of the section (and each paragraph). For<br>details, see <a href="http://www.learnnc.org/lp/editions/few/683">http://www.learnnc.org/lp/editions/few/683</a>.</li><li>A good way to demonstrate the logic is to use the first sentence in each paragraph to overview the<br>key message of the paragraph.</li><li>If a reader just reads the first sentences of all paragraphs in a single section, he should be able to<br>understand the key logic flow of the entire section. If this is not the case, then we need to change<br>(i) choose a better key message for each paragraph, and/or (ii) reorder the paragraphs to<br>demonstrate a better logic.</li><li>At the end of the section, summarize the key results, and provide a smooth transition to the next<br>section.<br>(3) If a section is too long, divide it into several subsections (which can be further divided into<br>subsubsection). Apply (2) to each subsection (and subsubsection).<br>A good example:<br><a href="http://ncel.ie.cuhk.edu.hk/sites/default/files/p2803-gao.pdf">http://ncel.ie.cuhk.edu.hk/sites/default/files/p2803-gao.pdf</a><h3 id="How-to-organize-the-list-of-contributions"><a href="#How-to-organize-the-list-of-contributions" class="headerlink" title="How to organize the list of contributions"></a>How to organize the list of contributions</h3>How to prepare the list of contributions. We need to answer the following questions concisely:</li><li>Why the problem is important and practically relevant?</li><li>Why the model is novel and practically important? Does the model offer a new angel of looking<br>at the problem? Will solving the model lead to something significant in practice? Just saying that<br>nobody studied before is not enough. (There can be many trivial or wrong approaches that people<br>nobody studied before is not enough. (There can be many trivial or wrong approaches that people<br>have not tried.)</li><li>What are the technical challenges? This is not about how we solve it, but how difficult it is to<br>solve. For example, we may talk about the difficulty in dealing with non-convexity optimization<br>problems, non-smooth functions, or high complexity of computing the optimal solution or game<br>equilibrium. Just telling people that we have solved the problem is not enough.</li><li>What are our technical contributions? In particular, how we address the challenges, and in certain<br>cases whether the analysis technique can help solve other technical problems?</li><li>What are the key insights and surprising results? Listing all the results is not necessary and often<br>counter-productive. Instead, we should emphasize those that are counter-intuitive and that reveal<br>key strength of our analysis.<br>We will summarize the answers to the above questions in concise terms and sentences. We only<br>emphasize the key things, not every result that we obtain. Listing everything = listing nothing.<h3 id="How-to-organize-literature-review"><a href="#How-to-organize-literature-review" class="headerlink" title="How to organize literature review"></a>How to organize literature review</h3>My general suggestions for literature review:<br>(1) Need to cover the most important ones as much as possible.<br>(2) When there are too many papers to cover on a topic, we need to narrow down the topic so that<br>it is small enough and is related to the key model/contribution/methodology of our paper. Then<br>repeat (1) above.<br>(3) In the case that we have really narrowed down the topic but still cannot cover all the important<br>papers, we should explicitly say so (such as ‚Äúwe are not able to cover all the related literature due<br>to limited space‚Äù).<br>(4) A good way to avoid criticism from reviewers is to use ‚Äúe.g.‚Äù For example, we can say that<br>‚ÄúThe literature on topic X can be divided into two groups: those focusing on deterministic channels<br>(e.g., [1]-[5]) and those focusing on random channels (e.g., [6]-[10])‚Äù. Here [1]-[5] are the most<br>relevant recent key literature that we can find on the deterministic channel case, and they may cite<br>some of the previous literature that we do not have space to cover in this paper.<br>(5) At the end of each literature review paragraph, we need to clearly state the issue of the<br>literature and how our paper addresses this issue. If we have difficulty in writing this out, then we<br>are not reviewing the relevant literature.<h3 id="How-to-organize-a-section-with-many-different-concepts"><a href="#How-to-organize-a-section-with-many-different-concepts" class="headerlink" title="How to organize a section with many different concepts"></a>How to organize a section with many different concepts</h3>The System Model section is often of this nature, where we need to introduce the key notations and<br>mathematical expressions.<br>(1) Use the first paragraph to explain the high level ideas (roadmap) and the concepts (e.g.,<br>variables/parameters/functions) that we plan to introduce, in plain English without using math.<br>(2) For each of the following paragraphs, use a bold or italicized phrase at the beginning of the<br>paragraph to highlight key concept to be introduced in the paragraph.<br>paragraph to highlight key concept to be introduced in the paragraph.<br>(3) Use the first sentence of each paragraph to summarize/preview the key message of this<br>paragraph.<br>A good example: Section III.C of<br><a href="http://ncel.ie.cuhk.edu.hk/sites/default/files/Gao_CISS2016.pdf">http://ncel.ie.cuhk.edu.hk/sites/default/files/Gao_CISS2016.pdf</a><h3 id="How-to-organize-core-mathematical-results"><a href="#How-to-organize-core-mathematical-results" class="headerlink" title="How to organize core mathematical results"></a>How to organize core mathematical results</h3>We need to organize each part of the results based on propositions and theorems.<br>More specifically, for each key result, we will introduce it in four parts: Preparation-&gt;Result-<blockquote><p>Proof-&gt;Intuition.</p></blockquote></li><li>First, Preparation. Introduce the necessary notations and maybe the intuitions to state the<br>mathematical results. Keep the math minimum unless it is absolutely necessary.</li><li>Second, Result. State the key result in a mathematical a lemma/proposition/theorem (see my<br>another comment regarding how to choose one of them).</li><li>Third, Proof. Provide the proof (in a Proof environment) with mathematical details. Writing the<br>proof is like writing a separate section: unless it is very short, we should provide an overview<br>paragraph regarding the roadmap of the proof, and then divide the proof into several clearly<br>labeled steps (often using bold fonts or bullets, or divide into several subsections if the proof is<br>really long). Good example of organizing proofs: <a href="https://arxiv.org/pdf/1609.01961">https://arxiv.org/pdf/1609.01961</a><br>If the proof is long and is not important for understanding the key messages/contributions, we can<br>put the proof in the appendix. If the space is not enough, we can put the proof in the online<br>appendix. But keep in mind, each mathematical result should have a formal proof, and a reader<br>needs to know where to find the proof.</li><li>Fourth, Intuition. For important results, explain the insights from the results in more discussions.<br>This is not required for Lemmas and some Propositions, but should be done for all Theorems (as<br>they are the core results of the paper). This is often where we emphasize the key contributions of<br>our work (hence readers will pay special attention).<br>To put things more simply, even the most mathematical part of the paper is still part of the ‚Äústory‚Äù.<br>The readers are here to understand the story, not to get into the mathematical details. Anything that<br>get in the way of storytelling should be simplified or removed from the main text.<br>Some good examples of how to present complicated mathematical results in a clean fashion:<br><a href="http://ncel.ie.cuhk.edu.hk/sites/default/files/p406-yu.pdf">http://ncel.ie.cuhk.edu.hk/sites/default/files/p406-yu.pdf</a><br><a href="http://ncel.ie.cuhk.edu.hk/sites/default/files/p2052-yu.pdf">http://ncel.ie.cuhk.edu.hk/sites/default/files/p2052-yu.pdf</a><h3 id="How-to-use-key-questions-to-emphasize-key-messages"><a href="#How-to-use-key-questions-to-emphasize-key-messages" class="headerlink" title="How to use key questions to emphasize key messages"></a>How to use key questions to emphasize key messages</h3>A technical paper needs to bring key messages out, loud and clear, to the readers. There are<br>different ways of doing that. A simple and effective way is to ask key questions and answer them<br>in the paper.</li></ul><ol><li>Where to ask questions: the Introduction section.</li><li>How many questions to ask: a single key question or several very closely related key questions.</li><li>What questions to ask: the questions need to be well motivated and reflect your goal of writing<br>this paper. The answers to the questions should be the key insights (take-home messages) that you<br>want to convey.</li><li>How to answer these questions: All the analysis and numerical results should directly (or<br>indirectly) answer these questions. In other words, anything that is not helpful in answering these<br>questions should be removed (either entirely or to the appendix, in case of independent interest to<br>the readers). It is worthwhile giving the answers to the questions explicitly somewhere in the paper<br>(e.g., ‚ÄúThis answers Question 1 mentioned in Introduction: ‚Ä¶.‚Äù).<h3 id="How-to-organize-the-simulation-results"><a href="#How-to-organize-the-simulation-results" class="headerlink" title="How to organize the simulation results"></a>How to organize the simulation results</h3>Before presenting the results, we should tell the readers what we want to examine and why it is<br>important. (If there is a way to very concisely highlight the key findings, we can also do this before<br>the detailed discussions.)<br>Then we need to think about what are the key messages that we want to deliver for each figure (or<br>a set of figures). Typically this can be captured by a short phrase at the beginning of the paragraph<br>(such as ‚ÄúThe impact of A on B) and a conclusion sentence at the end of the paragraph. This will<br>guide the readers‚Äôs focus and make sure that they get the right messages.<br>When we introduce the observation from a figure, we first need to explain what the figure is,<br>especially the figure contains more than one curve or involve complicated legends. Then for each<br>observation, we need to point the readers to the specific curve/point/data range of the particular<br>figure.<br>For the simulation results, the most important thing is to derive key insight. For example, the<br>reason behind the observation. Without the insight, the observation is often not very useful.<br>In fact, we can make many observations from the same figure. Some lead to meaningful insights<br>while other do not. After we decide what insights to emphasize (which should help us answer the<br>key questions in the introduction), then we can decide what observations to report. This is ‚Äúreverse<br>engineering technical writing‚Äù.<br>To summarize, for each figure (or a cluster of figures), we will<br>(1) Motivate: what are the main issues to examine.<br>(2) Observation: what interesting/surprising results we have observed.<br>(3) Insights: the explanations behind the observations, and whether they answer some key<br>questions in this paper.<h3 id="How-to-present-figures"><a href="#How-to-present-figures" class="headerlink" title="How to present figures"></a>How to present figures</h3>Each figure should be very clear when printed out black and white. In particular:</li></ol><ul><li>All texts in the figure (including labels and legends) should have large enough fonts.</li><li>Use dark colors: black, blue, red, purple, dark green, etc. Avoid light colors: green, yellow, etc.</li><li>Any two curves should not just be differentiated by color, as they might look exactly the same<br>when printed out black and white.</li><li>When two curves (partially) over with each other, need to use markers to differentiate them<br>instead of just line types.<h3 id="How-to-Present-Assumptions"><a href="#How-to-Present-Assumptions" class="headerlink" title="How to Present Assumptions"></a>How to Present Assumptions</h3>Almost all theoretical analysis are derived based on some key assumptions of the model. We need<br>to make these assumptions very clear. There are a few possibilities of stating these assumptions:</li><li>Make them very clear in the system model section.</li><li>Make them really stand out when we state them in later sections (if not introduced in the system<br>model section).<br>Usually it is a good idea to use ‚ÄúAssumption‚Äù environment, so that we can easily refer them in later<br>results, such as<br>Theorem 1: Under Assumption 1, we have ‚Ä¶<br>Remember to justify every assumption with reference support (can be academic paper, news report,<br>or website), no matter how small or trivial the assumption may seem (as the reviewers can have<br>very different opinions).<br>When the assumption is strong, we should acknowledge it explicitly (instead of hand-waiving on<br>this). Then we need to<br>(1) Explain why we make such a strong assumption. For example, it is necessary to derive the<br>closed-form solution, or the problem is already very challenging to solve even under such a<br>simplified/strong assumption, or it is true in some limited but very important practical cases.<br>(2) Explain the impact of such an assumption on the results. For example, which results/insights<br>might change if we relax this assumption, and which results/insights will remain unchanged. Note<br>that we need to be truthful here, and cannot speculate things that we do not know. For example,<br>unless we know that it is true, we can not say something like ‚ÄúThe main results will remain the<br>same if we relax this assumption.‚Äù<br>(3) Explain how we plan to relax such an assumption and what approaches that we will take.<br>For (2) and (3), we may explain in a footnote or at the end of the paper (often in the conclusion<br>section).<h3 id="How-to-define-mathematical-notations"><a href="#How-to-define-mathematical-notations" class="headerlink" title="How to define mathematical notations"></a>How to define mathematical notations</h3>It is a good idea to adopt the common notation convention:</li><li>Calligraphy letters to denote sets: mathcal{S}.</li><li>Capital non-bold letters to denote constants (not optimization variables): M.</li><li>Capital bold letters for matrix: oldsymbol{A}.</li><li>Lower case non-bold letters for variables: x.</li><li>Lower case bold letters for vectors: oldsymbol{x}.</li><li>Some lower case symbols are reserved for indices: i, j, k, l, m, n.<br>General guidelines for notations:</li><li>Each notation should have a unique meaning.</li><li>No two notations mean the same thing.</li><li>Following the notation conventions (some are explained above).</li><li>Choose the notations to reflect the physical meanings as much as possible.<h3 id="How-to-use-latexdiff-to-‚Äútrack-changes‚Äù-in-LaTex"><a href="#How-to-use-latexdiff-to-‚Äútrack-changes‚Äù-in-LaTex" class="headerlink" title="How to use latexdiff to ‚Äútrack changes‚Äù in LaTex"></a>How to use latexdiff to ‚Äútrack changes‚Äù in LaTex</h3>Comparing with Microsoft Word, a major disadvantage of using LaTex for technical writing is that<br>it is difficult to track changes. This has been (largely) solved by a Perl script called ‚Äúlatexdiff‚Äù:<br><a href="https://www.sharelatex.com/blog/2013/02/16/using-latexdiff-for-marking-changes-to-texdocuments">https://www.sharelatex.com/blog/2013/02/16/using-latexdiff-for-marking-changes-to-texdocuments</a>.<br>html<h2 id="Other-Tips"><a href="#Other-Tips" class="headerlink" title="Other Tips "></a>Other Tips <a name="tips"></a></h2><h3 id="Use-active-voice-as-much-as-possible-most-important-for-all-beginners"><a href="#Use-active-voice-as-much-as-possible-most-important-for-all-beginners" class="headerlink" title="Use active voice as much as possible (most important for all beginners)"></a>Use active voice as much as possible (most important for all beginners)</h3>When in doubt, use active voice, until you master the difference between active and passive voices.<br><a href="https://medium.com/@DaphneWatson/technical-writing-active-vs-passive-voice-485dfaa4e498">https://medium.com/@DaphneWatson/technical-writing-active-vs-passive-voice-485dfaa4e498</a><h3 id="How-to-start-a-sentence-in-the-literature-review"><a href="#How-to-start-a-sentence-in-the-literature-review" class="headerlink" title="How to start a sentence in the literature review?"></a>How to start a sentence in the literature review?</h3>Do not start a sentence with just a reference number. Instead of<br>‚Äú[6] proposed ‚Ä¶‚Äù<br>we can say<br>‚ÄúReference [6] proposed ‚Ä¶‚Äù<br>In fact, it is often much better to list the last names of the authors in the literature review. This<br>shows the respect to these authors.<br>Hence we should say<br>‚ÄúHa emph{et al.} in [6] proposed ‚Ä¶‚Äù<br>If there are only two authors, we can list both of their last names, like<br>‚ÄúJohn and Smith in [6] proposed ‚Ä¶‚Äù<h3 id="Why-we-should-define-notations-before-using-it"><a href="#Why-we-should-define-notations-before-using-it" class="headerlink" title="Why we should define notations before using it"></a>Why we should define notations before using it</h3>Define term before using it. It is a bad habit of doing the other way around.<br>Take this case as an example. Since a reader does not know what function pi_i() is, he cannot<br>understand (18) when first reading it. He will feel confused, and then read the definitions after the<br>equations, and then come back to the equation to understand it. This requires a lot of efforts from<br>the reader.<br>Our goal is to minimize the readers‚Äô effort in understanding the paper. In particular, we should<br>minimize the need for the reader to read ‚Äúnonlinearly‚Äù, i.e., back and forth in order to understand<br>things.<h3 id="Good-looking-Tables"><a href="#Good-looking-Tables" class="headerlink" title="Good looking Tables"></a>Good looking Tables</h3>Create better looking table using booktabs<br><a href="https://ctan.org/pkg/booktabs?lang=en">https://ctan.org/pkg/booktabs?lang=en</a><br><a href="https://en.wikibooks.org/wiki/LaTeX/Tables">https://en.wikibooks.org/wiki/LaTeX/Tables</a> (section 11: Professional tables, for an example)<br>More information:<br><a href="https://tex.stackexchange.com/questions/112343/beautiful-table-samples">https://tex.stackexchange.com/questions/112343/beautiful-table-samples</a><br><a href="http://www.inf.ethz.ch/personal/markusp/teaching/guides/guide-tables.pdf">http://www.inf.ethz.ch/personal/markusp/teaching/guides/guide-tables.pdf</a><h3 id="How-to-format-a-multiline-equation"><a href="#How-to-format-a-multiline-equation" class="headerlink" title="How to format a multiline equation"></a>How to format a multiline equation</h3>Use<br>egin{multline]<br>end{multline}<br>for better automatic formatting of multiple line equations.<h3 id="Clean-up-the-reference-format"><a href="#Clean-up-the-reference-format" class="headerlink" title="Clean up the reference format"></a>Clean up the reference format</h3>Consider the following reference:<br>Q. Ma, Y.-F. Liu, and J. Huang, ‚ÄúTime and location aware mobile data pricing,‚Äù in<br>emph{Proceedings of IEEE International Conference on Communications}, Sydney, Australia,<br>2014, pp. 3235‚Äì3240.<br>If space is limited, we can shorten it as (which is usually enough for a conference submission):<br>Q. Ma, Y.-F. Liu, and J. Huang, ‚ÄúTime and location aware mobile data pricing,‚Äù in emph{IEEE<br>ICC}, 2014.<br>Make sure that all the references have the consistent style in the same paper.<h3 id="When-to-capitalize-a-section-or-chapter"><a href="#When-to-capitalize-a-section-or-chapter" class="headerlink" title="When to capitalize a section or chapter"></a>When to capitalize a section or chapter</h3>Consider: ‚ÄúIn Chapter 3, it was shown that‚Ä¶‚Äù<br>This seems correct. ‚ÄúChapter 3‚Äù is the name of the third chapter. Names are capitalised.<br>Consider: ‚ÄúIn the previous Section, a method was presented to‚Ä¶‚Äù<br>This seems wrong. ‚ÄúSection‚Äù is not referring to the previous section by name, therefore no capital.<br>Consider: ‚ÄúThe graph in Figure 3 shows‚Ä¶‚Äù<br>Correct. Same as the first example.<br>So the rule (I use) is, if it is a proper name, then use a capital. This means, if it is of the form<br>‚ÄúSection $n$‚Äù, where $n$ is a number, then it needs a capital.<br><a href="http://academia.stackexchange.com/questions/9454/capitalisation-of-section-and-chapter-in-a-phd-">http://academia.stackexchange.com/questions/9454/capitalisation-of-section-and-chapter-in-a-phd-</a><br>thesis<h3 id="How-to-compare-two-things"><a href="#How-to-compare-two-things" class="headerlink" title="How to compare two things"></a>How to compare two things</h3>It is problematic to say<br>A user with a high heta is more likely ‚Ä¶<br>Reason: ‚ÄúMore‚Äù is used to comparison. However, ‚Äúa user with a high heta‚Äù only refers to a<br>particular user without stating the proper comparison group. Are we comparing with medium value<br>of heta, or small value of heta?<br>How to fix this?</li><li>Possibility 1: State the comparison group clearly:<br>‚ÄúComparing with A, B is more likely to ‚Ä¶‚Äù</li><li>Possibility 2: explain how A changes with B:<br>‚ÄúAs heta increases, a user is more likely to choose T_1.‚Äù<h3 id="Lemma-Proposition-Theorem-Corollary"><a href="#Lemma-Proposition-Theorem-Corollary" class="headerlink" title="Lemma/Proposition/Theorem/Corollary?"></a>Lemma/Proposition/Theorem/Corollary?</h3></li><li>A Theorem is a major result that you care about (e.g. ‚Äúthe goal of this paper is to prove the<br>following theorem‚Äù).</li><li>A Lemma is a useful result that needs to be invoked to prove some Theorem or other later on in<br>the paper.</li><li>A Proposition is a technical result that does not need to be invoked as a Lemma.</li><li>A Corollary is a short statement that is derived directly from a technical result (often a Theorem,<br>sometimes a Proposition) that has been proved in the paper. A corollary can often be viewed as a<br>‚Äúspecial case‚Äù of an existing result, for the purpose to illustrate/highlight something interesting.<br><a href="https://www.youtube.com/watch?v=Us0ZyTXVeAY">https://www.youtube.com/watch?v=Us0ZyTXVeAY</a><h3 id="Acronyms"><a href="#Acronyms" class="headerlink" title="Acronyms"></a>Acronyms</h3>Once we define an acronym, we should use it consistently throughout the paper, and do not switch<br>back to the full expression. The only exception is the conclusion session, where we can use the full<br>expression again.<br>Alternatively, if the full expression is not long, we do not need to define the acronym. Keeping the<br>full expression is usually better for readability.<br>The most important thing is to be consistent.<h3 id="Table-caption"><a href="#Table-caption" class="headerlink" title="Table caption"></a>Table caption</h3>In a table, the caption is typically above the table. In a figure, the caption is typically below the<br>figure.<br><a href="http://tex.stackexchange.com/questions/3243/why-should-a-table-caption-be-placed-above-thetable">http://tex.stackexchange.com/questions/3243/why-should-a-table-caption-be-placed-above-thetable</a><h3 id="Location-of-footnote"><a href="#Location-of-footnote" class="headerlink" title="Location of footnote"></a>Location of footnote</h3>The footnote should be placed outside the punctuation.<br><a href="https://english.stackexchange.com/questions/95199/do-footnoting-superscripts-go-inside-oroutside-">https://english.stackexchange.com/questions/95199/do-footnoting-superscripts-go-inside-oroutside-</a><br>punctuation<h3 id="Do-not-start-a-sentence-with-‚ÄúI-e-‚Äù-or-‚ÄúE-g-‚Äù"><a href="#Do-not-start-a-sentence-with-‚ÄúI-e-‚Äù-or-‚ÄúE-g-‚Äù" class="headerlink" title="Do not start a sentence with ‚ÄúI.e.‚Äù or ‚ÄúE.g.‚Äù:"></a>Do not start a sentence with ‚ÄúI.e.‚Äù or ‚ÄúE.g.‚Äù:</h3><a href="https://english.stackexchange.com/questions/30106/can-i-start-a-sentence-with-i-e">https://english.stackexchange.com/questions/30106/can-i-start-a-sentence-with-i-e</a><h3 id="Oxford-Comma"><a href="#Oxford-Comma" class="headerlink" title="Oxford Comma"></a>Oxford Comma</h3>Instead of writing<br>‚ÄúPlease bring me a pencil, eraser and notebook.‚Äù<br>write<br>‚ÄúPlease bring me a pencil, eraser, and notebook.‚Äù<br><a href="https://www.grammarly.com/blog/what-is-the-oxford-comma-and-why-do-people-care-so-muchabout-">https://www.grammarly.com/blog/what-is-the-oxford-comma-and-why-do-people-care-so-muchabout-</a><br>it/<h3 id="Notation-or-Notations"><a href="#Notation-or-Notations" class="headerlink" title="Notation or Notations"></a>Notation or Notations</h3>Seems that ‚ÄúNotations‚Äù would be fine.<br><a href="https://english.stackexchange.com/questions/186434/is-notations-a-proper-english-word">https://english.stackexchange.com/questions/186434/is-notations-a-proper-english-word</a><br>Globally check and revise.<h3 id="Using-‚Äúa‚Äù-or-‚Äúan‚Äù-With-Acronyms-and-Abbreviations"><a href="#Using-‚Äúa‚Äù-or-‚Äúan‚Äù-With-Acronyms-and-Abbreviations" class="headerlink" title="Using ‚Äúa‚Äù or ‚Äúan‚Äù With Acronyms and Abbreviations"></a>Using ‚Äúa‚Äù or ‚Äúan‚Äù With Acronyms and Abbreviations</h3>The general rule for indefinite articles is to use a before consonants and an before vowels. The trick<br>here is to use your ears (how the acronym is pronounced), not your eyes (how it‚Äôs spelled). HIV<br>(pronounced ‚Äúaitch eye vee‚Äù) begins with a vowel sound, so an HIV patient is correct. HIPAA<br>(pronounced ‚Äúhippa‚Äù) begins with a consonant sound, so a HIPAA form is correct.<br><a href="http://blog.apastyle.org/apastyle/2012/04/using-a-or-an-with-acronyms-and-abbreviations.html">http://blog.apastyle.org/apastyle/2012/04/using-a-or-an-with-acronyms-and-abbreviations.html</a><h3 id="Because-vs-Because-of"><a href="#Because-vs-Because-of" class="headerlink" title="Because vs. Because of"></a>Because vs. Because of</h3><a href="https://www.youtube.com/watch?v=WbeaSDAphQk">https://www.youtube.com/watch?v=WbeaSDAphQk</a><h3 id="Avoid-mixing-formal-mathematical-language-and-natural-language"><a href="#Avoid-mixing-formal-mathematical-language-and-natural-language" class="headerlink" title="Avoid mixing formal mathematical language and natural language"></a>Avoid mixing formal mathematical language and natural language</h3><a href="https://math.stackexchange.com/questions/1530342/about-the-position-of-for-all-quantifier">https://math.stackexchange.com/questions/1530342/about-the-position-of-for-all-quantifier</a><h3 id="Cross-referencing-sections-and-equations"><a href="#Cross-referencing-sections-and-equations" class="headerlink" title="Cross referencing sections and equations"></a>Cross referencing sections and equations</h3>Always use the proper LaTex command to properly cross reference objectives, so that the<br>references are automatically generated. NEVER hard-code the cross references. This will make<br>things a mess when you revise the paper.<br><a href="https://www.sharelatex.com/learn/Cross_referencing_sections_and_equations">https://www.sharelatex.com/learn/Cross_referencing_sections_and_equations</a><h3 id="How-to-use-‚Äúa-an-the‚Äù"><a href="#How-to-use-‚Äúa-an-the‚Äù" class="headerlink" title="How to use ‚Äúa/an/the‚Äù?"></a>How to use ‚Äúa/an/the‚Äù?</h3><a href="https://owl.english.purdue.edu/owl/resource/540/01/">https://owl.english.purdue.edu/owl/resource/540/01/</a><br>Usually we need to have an article before a singular word, with only few exceptions.<br><a href="http://www.getitwriteonline.com/archive/tips.htm">http://www.getitwriteonline.com/archive/tips.htm</a><br><a href="http://www.quickanddirtytips.com/education/grammar/when-use-articles-nouns?page=all">http://www.quickanddirtytips.com/education/grammar/when-use-articles-nouns?page=all</a><h3 id="Include-punctuation-in-equations"><a href="#Include-punctuation-in-equations" class="headerlink" title="Include punctuation in equations"></a>Include punctuation in equations</h3><a href="https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Mathematics#Punctuation_after_formulae">https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Mathematics#Punctuation_after_formulae</a><h3 id="Using-rac-for-inline-math"><a href="#Using-rac-for-inline-math" class="headerlink" title="Using rac for inline math"></a>Using rac for inline math</h3>For inline match, instead of using rac{a}{b} which disrupts the proper line spacing, I recommend<br>using a/b.<br>There are some systematic ways of doing this (without manually changing the LaTex code every<br>time):<br><a href="https://tex.stackexchange.com/questions/317268/substitute-frac-for-a-slash-only-on-inline-mathmode/317272">https://tex.stackexchange.com/questions/317268/substitute-frac-for-a-slash-only-on-inline-mathmode/317272</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Technical-Writing&quot;&gt;&lt;a href=&quot;#Technical-Writing&quot; class=&quot;headerlink&quot; title=&quot;Technical Writing&quot;&gt;&lt;/a&gt;Technical Writing&lt;/h1&gt;&lt;p&gt;Prepared b</summary>
      
    
    
    
    <category term="research" scheme="https://luuvy757.github.io/categories/research/"/>
    
    
  </entry>
  
  <entry>
    <title>CSC4005 Parallel Computing</title>
    <link href="https://luuvy757.github.io/2022/03/28/CSC4005-Parallel-Computing/"/>
    <id>https://luuvy757.github.io/2022/03/28/CSC4005-Parallel-Computing/</id>
    <published>2022-03-28T09:13:55.000Z</published>
    <updated>2023-02-23T02:03:35.398Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Project-1-Odd-Even-Sort"><a href="#Project-1-Odd-Even-Sort" class="headerlink" title="Project 1: Odd Even Sort"></a>Project 1: Odd Even Sort</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><blockquote><p>sequential odd even sort $O(n^{2})$   </p></blockquote><img src="/2022/03/28/CSC4005-Parallel-Computing/odd_even.jpg" class=""><img src="/2022/03/28/CSC4005-Parallel-Computing/flow.jpg" class=""><h2 id="Method-‚Äì-MPI"><a href="#Method-‚Äì-MPI" class="headerlink" title="Method ‚Äì MPI"></a>Method ‚Äì MPI</h2><h3 id="Scatter-Numbers-MPI-Scatterv-‚Ä¶"><a href="#Scatter-Numbers-MPI-Scatterv-‚Ä¶" class="headerlink" title="Scatter Numbers (MPI_Scatterv(‚Ä¶))"></a>Scatter Numbers (MPI_Scatterv(‚Ä¶))</h3><ul><li>receive_count[i] <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">avg = floor(input_size / process_number)</span><br><span class="line">rem = input size % process number</span><br><span class="line">if rank i &lt; rem: </span><br><span class="line">    receive_count[i] = avg + 1; </span><br><span class="line">else:</span><br><span class="line">    receive_count[i] = avg;</span><br></pre></td></tr></table></figure></li></ul><h3 id="Communication-between-process-MPI-Sendrecv-‚Ä¶"><a href="#Communication-between-process-MPI-Sendrecv-‚Ä¶" class="headerlink" title="Communication between process (MPI_Sendrecv(‚Ä¶))"></a>Communication between process (MPI_Sendrecv(‚Ä¶))</h3><ul><li>find_partner(index, phase)</li><li>boundary check<ul><li>The current process receives nothing.</li><li>Partner is NOT in received numbers =&gt; need communication.</li><li>Partner is NOT in the range of the whole array, DO NOT need any comparison and exchange.</li></ul></li><li>communicate with neighbor process<img src="/2022/03/28/CSC4005-Parallel-Computing/communicate_neighbor.jpg" class=""></li></ul><h1 id="Project-2-Mandelbrot-Set"><a href="#Project-2-Mandelbrot-Set" class="headerlink" title="Project 2: Mandelbrot Set"></a>Project 2: Mandelbrot Set</h1><p>Report: <a href="https://github.com/luuvy757/CSC4005-As2-Mandelbrot-Set/blob/master/readme.pdf">https://github.com/luuvy757/CSC4005-As2-Mandelbrot-Set/blob/master/readme.pdf</a></p><h1 id="Project-3-N-Body-Simulation"><a href="#Project-3-N-Body-Simulation" class="headerlink" title="Project 3: N Body Simulation"></a>Project 3: N Body Simulation</h1><h2 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h2><p>An N-body simulation is a simulation of the evolution of N bodies in a system<br>based on the Newton‚Äôs Law (gravitational force). Each body continuously<br>interacts with every other body, and with collision. It approximates an astrophysical<br>simulation in which each body represents a galaxy or an individual<br>star. In this project, the 2D dimensional N-body simulation uses the all-pairs<br>approach that is a brute-force technique with $O(n^2)$computational complexity.</p><p>Sequential Algorithm:</p><ol><li>Step1: Calculate force interaction between all-pair bodies.<br>For a body i, update its acceleration as it interact with other bodies.<br>Besides, update its velocity and position as it collides with other bodies.</li><li>Step2: Simulate movement within elapse.<br>In this step, loop for every body to update its velocity and position.<br>$v<em>n = v</em>{n-1} + at, x<em>n = x</em>{n-1} + vt$</li></ol><h2 id="Method-‚Äì-MPI-1"><a href="#Method-‚Äì-MPI-1" class="headerlink" title="Method ‚Äì MPI"></a>Method ‚Äì MPI</h2><img src="/2022/03/28/CSC4005-Parallel-Computing/Nbody_mpi.jpg" class="">  <p>(a) Broadcast <code>BodyPool</code><br>  The master (rank 0) broadcast all vectors (ay, ax, x, y, vx, vy) to<br>  other processes, and other processes receive the BodyPool.<br>(b) Parallel Computation<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for (int i = start_body; i &lt; end_body; i++)&#123;</span><br><span class="line">  for (int j = 0; j &lt; nbody; j++)&#123;</span><br><span class="line">    update ax, ay, vx, vy of body i</span><br><span class="line">    handle body collision, update x, y of body i</span><br><span class="line">  &#125;</span><br><span class="line">  update x,y of body i within elapse</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="Method-‚Äì-OpenMP-Pthread"><a href="#Method-‚Äì-OpenMP-Pthread" class="headerlink" title="Method ‚Äì OpenMP/Pthread"></a>Method ‚Äì OpenMP/Pthread</h2><p>OpenMP is a shared memory approach which is similar to the Pthread<br>approach. The common problem of shared memory methods is the data<br>race. The data race occurs in the Step 1.<br>When the two bodies i and j <strong>collide</strong> with each other, the algorithm try<br>to read their velocity while update the body i‚Äôs velocity. Date race<br>happens when reading and writing velocity of the same body.</p><h2 id="Method-‚Äì-MPI-OpenMP"><a href="#Method-‚Äì-MPI-OpenMP" class="headerlink" title="Method ‚Äì MPI+OpenMP"></a>Method ‚Äì MPI+OpenMP</h2><p>Based on the implementation of MPI, the computational part is modified to OpenMP parallel computation. What‚Äôs more, the Step 1 and<br>Step 2 are separated as the data race may occur in OpenMP parallel<br>computation. The computational part of the MPI+OpenMP is largely<br>same as that of the OpenMP.</p><h2 id="Method-‚Äì-CUDA"><a href="#Method-‚Äì-CUDA" class="headerlink" title="Method ‚Äì CUDA"></a>Method ‚Äì CUDA</h2><p>In CUDA programming, the Uni‚ÄÄed Memory is used to eliminate the<br>need for explicit data movement via the <code>cudaMemcpy*()</code> routines without<br>the performance penalty incurred by placing all data into zero-copy<br>memory</p><h1 id="Project-4-Heat-Simulation"><a href="#Project-4-Heat-Simulation" class="headerlink" title="Project 4: Heat Simulation"></a>Project 4: Heat Simulation</h1><h2 id="Overview-2"><a href="#Overview-2" class="headerlink" title="Overview"></a>Overview</h2><img src="/2022/03/28/CSC4005-Parallel-Computing/heat_simulation.jpg" class=""><img src="/2022/03/28/CSC4005-Parallel-Computing/heat_simulation_1.jpg" class=""><h2 id="Method-‚Äì-MPI-2"><a href="#Method-‚Äì-MPI-2" class="headerlink" title="Method ‚Äì MPI"></a>Method ‚Äì MPI</h2><img src="/2022/03/28/CSC4005-Parallel-Computing/heat_mpi.jpg" class=""><img src="/2022/03/28/CSC4005-Parallel-Computing/ghost.jpg" class="">  <p>(a) Each process compute the workload (begin row, end row) based on the their rankID.<br>(b) Each process create a local grid with ghost points.<br>(c) Each process send result back to rank 0, and rank 0 gather the result to make decision finish or not.</p><h2 id="Method-‚Äì-OpenMP-Pthread-CUDA"><a href="#Method-‚Äì-OpenMP-Pthread-CUDA" class="headerlink" title="Method ‚Äì OpenMP/Pthread/CUDA"></a>Method ‚Äì OpenMP/Pthread/CUDA</h2><img src="/2022/03/28/CSC4005-Parallel-Computing/heat_thread.jpg" class="">  <p>The <strong>Pthread</strong> method is designed for shared memory. Each thread works<br>on certain rows (based on <code>threadId</code>) of grid and update the result on buffer grid. After<br>each iteration, the threads are <strong>joined</strong> and the grid will be switched.<br>Termination occurs when iteration reaches the given max iteration.<br><strong>OpenMP</strong> is a shared memory approach which is similar to the Pthread<br>approach. The different is the OpenMP is implicit scheduling.<br><strong>NO DATA RACE</strong> Need synchronize after each round, and then switch the buffer.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul><li>The performance of Pthread is better than OpenMP because the self-scheduling is not free.</li><li>OpenMP and Pthread is limited with the physical thread number per node, when the thread number exceeds 32, the speedup drops down.</li><li>For distribute memory. The communication overhead occurs when problem size is small.</li><li>In general, shared-memory approaches are better than distributed-memory approach, but the MPI method can used for multi-computer.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Project-1-Odd-Even-Sort&quot;&gt;&lt;a href=&quot;#Project-1-Odd-Even-Sort&quot; class=&quot;headerlink&quot; title=&quot;Project 1: Odd Even Sort&quot;&gt;&lt;/a&gt;Project 1: Odd E</summary>
      
    
    
    
    <category term="course" scheme="https://luuvy757.github.io/categories/course/"/>
    
    
  </entry>
  
  <entry>
    <title>CSC3150 Operating System</title>
    <link href="https://luuvy757.github.io/2022/03/25/CSC3150-Operating-System/"/>
    <id>https://luuvy757.github.io/2022/03/25/CSC3150-Operating-System/</id>
    <published>2022-03-24T16:24:13.000Z</published>
    <updated>2023-02-23T02:58:19.054Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Project-1-User-Thread-and-Kernel-Thread"><a href="#Project-1-User-Thread-and-Kernel-Thread" class="headerlink" title="Project 1: User Thread and Kernel Thread"></a>Project 1: User Thread and Kernel Thread</h1><p><a href="https://github.com/luuvy757/CSC3150_As1-Kernel-Mode-Multi-Process-Programming">https://github.com/luuvy757/CSC3150_As1-Kernel-Mode-Multi-Process-Programming</a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><blockquote><p>In computing, a loadable kernel module (LKM) is an object file that contains code to extend the running kernel, or so-called base kernel, of an operating system. LKMs are typically used to <strong>add support for new hardware (as device drivers) and/or filesystems, or for adding system calls</strong>. When the functionality provided by an LKM is no longer required, it can be unloaded in order to free memory and other resources.</p></blockquote><h2 id="Task-1-user-mode"><a href="#Task-1-user-mode" class="headerlink" title="Task 1: user mode"></a>Task 1: user mode</h2><div class="table-container"><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><img src="/2022/03/25/CSC3150-Operating-System/user_mode.jpg" class=""></td><td><img src="/2022/03/25/CSC3150-Operating-System/signals.jpg" class=""></td></tr></tbody></table></div><h3 id="In-user-mode-fork-a-child-process-to-execute-the-test-program"><a href="#In-user-mode-fork-a-child-process-to-execute-the-test-program" class="headerlink" title="In user mode, fork a child process to execute the test program."></a>In user mode, fork a child process to execute the test program.</h3><p><code>pid = fork()</code>: <code>pid = -1 (failed); pid = 0 (child process); pid &gt; 0 (parent pid)</code><br><code>getpid()</code>: get process id.</p><h3 id="Detect-exception-cases-generated-by-test-programs"><a href="#Detect-exception-cases-generated-by-test-programs" class="headerlink" title="Detect exception cases generated by test programs"></a>Detect exception cases generated by test programs</h3><p><code>waitpid(pid, &amp;status, WUNTRACED);</code><br><code>status</code> is the identifier.</p><h2 id="Task-2-kernel-mode"><a href="#Task-2-kernel-mode" class="headerlink" title="Task 2: kernel mode"></a>Task 2: kernel mode</h2><img src="/2022/03/25/CSC3150-Operating-System/kernel_mode.jpg" class=""><h3 id="Functions"><a href="#Functions" class="headerlink" title="Functions"></a>Functions</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">my_exec</span><span class="params">(<span class="keyword">void</span>)</span> ... </span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_wait</span><span class="params">(<span class="keyword">pid_t</span> pid)</span> ... </span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">my_fork</span><span class="params">(<span class="keyword">void</span> *argc)</span> ...</span></span><br></pre></td></tr></table></figure><h3 id="Create-kernel-module"><a href="#Create-kernel-module" class="headerlink" title="Create kernel module"></a>Create kernel module</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> __init <span class="title">program2_init</span><span class="params">(<span class="keyword">void</span>)</span></span>&#123;</span><br><span class="line">printk(<span class="string">&quot;[program2] : Module_init\n&quot;</span>);</span><br><span class="line"><span class="comment">/* write your code here */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">task_struct</span> *<span class="title">my_kthread</span>;</span></span><br><span class="line">printk(<span class="string">&quot;[program2] :module_init create kthread start&quot;</span>);</span><br><span class="line">    <span class="comment">/* create a kernel thread to run my_fork */</span></span><br><span class="line">my_kthread = kthread_create(&amp;my_fork, <span class="literal">NULL</span>, <span class="string">&quot;my_kthread&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!IS_ERR(my_kthread)) &#123;</span><br><span class="line">printk(<span class="string">&quot;[program2] : module_init kthread start&quot;</span>);</span><br><span class="line">wake_up_process(my_kthread);</span><br><span class="line">        <span class="comment">// wake_up_process() When the wake_up_process is called the function passed to kthread_create gets executed.</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Firstly, create a kernel thread that forks a process <code>struct task_struct *my_kthread =kthread_create(&amp;my_fork, NULL, &quot;my_kthread&quot;);</code>, and start execution<br><code>wake_up_process(my_kthread);</code>.<br>In <code>my_fork()</code>, a child process is created by <code>_do_fork()</code> to execute test programs<br><code>_do_fork(SIGCHLD, (unsigned long) &amp;my_exec, 0, NULL, NULL, 0);</code> . Besides,<br>use <code>do_wait(&amp;wo)</code> to receive signal from the child process. After reveiving the signal, print out<br>the signal messages and child process terminated information. In <code>my_exec()</code>, use do_execve to<br>execute a file where raises signals.</p><h3 id="Insert-kernel-module-and-remove-kernel-module"><a href="#Insert-kernel-module-and-remove-kernel-module" class="headerlink" title="Insert kernel module and remove kernel module"></a>Insert kernel module and remove kernel module</h3><p><code>inmod &amp; rmmod</code> </p><hr><h1 id="Project-2-Frog-Game-using-pthread"><a href="#Project-2-Frog-Game-using-pthread" class="headerlink" title="Project 2: Frog Game (using pthread)"></a>Project 2: Frog Game (using pthread)</h1><blockquote><p><strong>thread</strong>:<br>A process can have multiple threads. The threads share the resources within a process and they<br>execute within the same address space. The shared resources can be accessed by all the threads, so that the<br>threads can write and read the shared resources. Therefore, it is significant to make explicit synchronization by<br>the programmer.<br><strong>critical section</strong>: The critical section is a code segment where the shared variables can be accessed. An atomic action is required in a critical section i.e. only one process can execute in its critical section at a time. All the other processes have to wait to execute in their critical sections.<br><strong>mutex</strong>: a data structure</p></blockquote><h2 id="Mutex-Lock"><a href="#Mutex-Lock" class="headerlink" title="Mutex Lock"></a>Mutex Lock</h2><ul><li><p><code>print_map()</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print_map</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    pthread_mutex_lock(&amp;mutex);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\033[H\033[2J&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= ROW; i++)</span><br><span class="line">        <span class="built_in">puts</span>(<span class="built_in">map</span>[i]);</span><br><span class="line">    usleep(<span class="number">1e4</span>);</span><br><span class="line">    pthread_mutex_unlock(&amp;mutex);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The shared data map is accessed by the <code>print_map()</code> in the <code>frog_control</code> thread. However, in the<br><code>logs_move()</code> of logs control threads, the shared data map is changed. To avoid the conflicts, the<br>mutex is required to make a critical section in the <code>print_map()</code>.</p></li><li><p><code>frog_control()</code></p><p>  The shared data frog is changed when the player controlling it. However, in the <code>logs_move()</code> of logs<br>  control threads, the frog is changed as it moving along with the log. Thus, to avoid the conflicts, the<br>  mutex is required to make a critical section in the <code>frog_control()</code>.</p></li></ul><hr><h1 id="Project-3-Virtual-Memory-Simulation"><a href="#Project-3-Virtual-Memory-Simulation" class="headerlink" title="Project 3: Virtual Memory Simulation"></a>Project 3: Virtual Memory Simulation</h1><p><a href="https://github.com/luuvy757/CSC3150-As3-Vitrual-Memory-Simulation">https://github.com/luuvy757/CSC3150-As3-Vitrual-Memory-Simulation</a></p><h2 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h2><p>  The virtual memory is efficient to execute partially-loaded program, so that the program is free of physical memory limited space. The virtual address is generated by the CPU, and after loading, the physical address is bound. The virtual memory space can be divided into pages, and the physical memory space can be divided into frames of the same size. If there is a reference to a page, OS needs to check the page table to find out its physical page number or it is not in memory, where the page fault may occur. The virtual memory management is significant in mapping virtual memory to physical memory, and load data from disk into physical memory only when necessary.<br>  <img src="/2022/03/25/CSC3150-Operating-System/GPU.jpg" class="" title="Figure 1.1"><br>  In this project, we aim to simulate a virtual memory management in GPU, and the Fig.1.1 shows how the memory arrangement in the GPU. Moreover, I used Inverted Page Table(IPT) in this project, and LRU is implemented for swapping out when page faults.<br><img src="/2022/03/25/CSC3150-Operating-System/vm_read.png" class=""></p><h2 id="Transform-Virtual-address-to-Physical-address"><a href="#Transform-Virtual-address-to-Physical-address" class="headerlink" title="Transform Virtual address to Physical address"></a>Transform Virtual address to Physical address</h2><ul><li>In this project, <strong>virtual memory space</strong> and <strong>disk memory space</strong> is one-to-one mapping.</li></ul><ol><li>Given virtual address, we can get page number, and page offset. </li><li>Linearly search Invert Page Table<br>(a) exist =&gt; frame number = current index;<br>(b) not exist =&gt; page fault =&gt; handle page fault =&gt; get frame number</li><li>Physical address = frame number*page_size + page offset</li><li>Don‚Äôt forget to update LRU set!<h2 id="Page-Fault-Handling"><a href="#Page-Fault-Handling" class="headerlink" title="Page Fault Handling"></a>Page Fault Handling</h2></li></ol><ul><li>LRU set contains all occupied frames.</li><li>Target: Find frame and put virtual page into physical frame.</li></ul><ol><li>Check if there is a free frame<br>(a) Yes, return free frame.<br>(b) No, LRU =&gt; swap out and return LRU frame </li><li>Swap virtual page into physical frame. (virtual address and disk address is one-to-one mapping)</li></ol><h2 id="Invert-Page-Table"><a href="#Invert-Page-Table" class="headerlink" title="Invert Page Table"></a>Invert Page Table</h2><ul><li>#entries = $\frac{physicalMemorySize (32KB)}{frame Size (32B)} = 2^{10} = 1024$ </li><li><em>frame number-&gt; page number</em> </li><li><strong>Linear search</strong> to find whether page number is already in physical memory.</li><li>Why Linear Search? For Table in the structure of <code>page number -&gt; frame number</code>, we can access index(page number) to check the frame number is valid or not.</li></ul><h2 id="Page-Replacement-LRU"><a href="#Page-Replacement-LRU" class="headerlink" title="Page Replacement (LRU)"></a>Page Replacement (LRU)</h2><ul><li>data structure<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  u16 prev;</span><br><span class="line">  u16 next;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">LRU</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  Node* head;  <span class="comment">//nodes[0] (sentinel)</span></span><br><span class="line">  Node* tail;  <span class="comment">//nodes[1025] (sentinel)</span></span><br><span class="line">  Node* nodes;</span><br><span class="line"></span><br><span class="line">  u16 count = <span class="number">0</span>;  </span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></li><li>k = nodes[i].prev: (i-1)th frame‚Äôs prev is (k-1)th frame.</li><li>k = nodes[i].next: (i-1)th frame‚Äôs next is (k-1)th frame.</li><li>ith frame is free &lt;=&gt; nodes[i+1].prev is <code>null</code></li><li>LRU frame number= <code>LRU-&gt;tail.prev - 1</code></li><li>update LRU: <ul><li>(a) There is a free frame: add this free frame number into LRU (doubly-link-list, add operation at <code>head</code>) </li><li>(b) Physical memory is full: return the LRU frame. -&gt; delete operation &amp; add operation</li></ul></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*virtual_memory.h*/</span></span><br><span class="line"><span class="function">__device__ <span class="keyword">void</span> <span class="title">update_LRU</span><span class="params">(VirtualMemory* vm, u16 frame_number)</span></span>;</span><br><span class="line"><span class="function">__device__ u16 <span class="title">get_LRU_frame_number</span><span class="params">(VirtualMemory* vm)</span></span>;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*virtual_memory.cu*/</span></span><br><span class="line"><span class="function">__device__ <span class="keyword">void</span> <span class="title">init_LRU</span><span class="params">(VirtualMemory *vm)</span></span>;</span><br><span class="line"><span class="function">__device__ <span class="keyword">void</span> <span class="title">add_to_LRU</span><span class="params">(VirtualMemory *vm, u16 frame_number)</span></span>;</span><br><span class="line"><span class="function">__device__ <span class="keyword">void</span> <span class="title">update_LRU</span><span class="params">(VirtualMemory *vm, u16 frame_number)</span></span>;</span><br><span class="line"><span class="function">__device__ u16 <span class="title">get_LRU_frame_number</span><span class="params">(VirtualMemory *vm)</span></span></span><br></pre></td></tr></table></figure><p><code>init_LRU</code> is used to initialize the LRU‚Äìputting the LRU into the virtual memory. \<br><code>add_to_LRU</code> is used to add node in the front of the doubly linked list. \<br><code>update_LRU</code> is used to update the LRU once a frame is accessed. \<br><code>get_LRU_frame_number</code> is used to get the <em>least recently unused</em> <code>frame_number</code>, which will be swapped out later.</p><hr><h1 id="Project-4-File-System-Simulation"><a href="#Project-4-File-System-Simulation" class="headerlink" title="Project 4: File System Simulation"></a>Project 4: File System Simulation</h1><h2 id="Overview-2"><a href="#Overview-2" class="headerlink" title="Overview"></a>Overview</h2><p><a href="https://github.com/luuvy757/CSC3150-As4-File-System">https://github.com/luuvy757/CSC3150-As4-File-System</a></p><ul><li>Layered File System<img src="/2022/03/25/CSC3150-Operating-System/layered_fs.png" class=""></li><li>Volume Control Block: total # of blocks, # of free blocks‚Ä¶ </li><li>File Control Block: file size, time, filename‚Ä¶<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="Data-structure"><a href="#Data-structure" class="headerlink" title="Data structure"></a>Data structure</h3></li></ul><ol><li>Super Block<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">SuperBlock</span></span></span><br><span class="line"><span class="class"> &#123;</span></span><br><span class="line">     <span class="keyword">int</span> free_block_count; <span class="comment">// how many free block</span></span><br><span class="line">     u16 free_block_start; <span class="comment">// the first start free block number;</span></span><br><span class="line">     <span class="keyword">int</span> total_file = <span class="number">0</span>;   <span class="comment">// how many files in the storge</span></span><br><span class="line"> &#125;;</span><br></pre></td></tr></table></figure></li><li><p>File Control Block  </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> pack(1)</span></span><br><span class="line"> <span class="class"><span class="keyword">struct</span> <span class="title">FCB</span></span></span><br><span class="line"><span class="class"> &#123;</span></span><br><span class="line">     u32 modified_time; <span class="comment">// 4 bytes</span></span><br><span class="line">     u32 create_time;   <span class="comment">// 4 bytes</span></span><br><span class="line">     u16 file_size;     <span class="comment">// 2 bytes</span></span><br><span class="line">     u16 start_block;</span><br><span class="line">     <span class="keyword">char</span> filename[<span class="number">20</span>];</span><br><span class="line"> &#125;;</span><br><span class="line"> <span class="meta">#<span class="meta-keyword">pragma</span> pack()</span></span><br></pre></td></tr></table></figure><p>Declaration in the File System: </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SuperBlock *superBlock_ptr;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">FCB</span> *<span class="title">FCB_arr</span>;</span></span><br><span class="line">uchar *fileContent_ptr;</span><br></pre></td></tr></table></figure><h2 id="APIs"><a href="#APIs" class="headerlink" title="APIs"></a>APIs</h2><h3 id="fs-open-fs-name-G-READ-G-WRITE"><a href="#fs-open-fs-name-G-READ-G-WRITE" class="headerlink" title="fs_open(fs, name, G_READ/G_WRITE)"></a><code>fs_open(fs, name, G_READ/G_WRITE)</code></h3><img src="/2022/03/25/CSC3150-Operating-System/fs_open.png" class=""><h3 id="fs-write-fs-input-size-fp"><a href="#fs-write-fs-input-size-fp" class="headerlink" title="fs_write(fs, input, size, fp)"></a><code>fs_write(fs, input, size, fp)</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (u32 i = start_addr; i &lt; start_addr + size; i++)</span><br><span class="line">&#123;</span><br><span class="line">    fs-&gt;fileContent_ptr[i] = input[i - start_addr];</span><br><span class="line">&#125; <span class="comment">// write to File Content from input buffer.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// update FCB_arr</span></span><br><span class="line">fs-&gt;FCB_arr[fp].file_size = size;</span><br><span class="line">fs-&gt;FCB_arr[fp].modified_time = gtime++;</span><br><span class="line"></span><br><span class="line"><span class="comment">// update superBlock_ptr</span></span><br><span class="line"><span class="keyword">int</span> delta_block = block_needs - origin_blocks;</span><br><span class="line">fs-&gt;superBlock_ptr-&gt;free_block_count -= delta_block;</span><br><span class="line">fs-&gt;superBlock_ptr-&gt;free_block_start += delta_block;</span><br></pre></td></tr></table></figure></li></ol><h3 id="fs-read-fs-output-size-fp"><a href="#fs-read-fs-output-size-fp" class="headerlink" title="fs_read(fs, output, size, fp)"></a><code>fs_read(fs, output, size, fp)</code></h3><ul><li><p>fp =&gt; get <code>start_block</code> and <code>file_size</code></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u16 start_block = fs-&gt;FCB_ar[fp].start_block;</span><br><span class="line">u16 file_size = fs-&gt;FCB_arr[fp].file_size;</span><br></pre></td></tr></table></figure></li><li>Get physical start address and end address<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u32 start_addr = start_block * fs-&gt;STORAGE_BLOCK_SIZE;</span><br><span class="line">u32 end_addr = start_addr + size;</span><br></pre></td></tr></table></figure></li><li>Read from File Content, and put result to <code>output</code> buffer.</li></ul><h3 id="fs-gsys-fs-RM-name"><a href="#fs-gsys-fs-RM-name" class="headerlink" title="fs_gsys(fs,RM, name)"></a><code>fs_gsys(fs,RM, name)</code></h3><img src="/2022/03/25/CSC3150-Operating-System/fs_remove.png" class=""><h3 id="fs-gsys-LS-D-LS-S"><a href="#fs-gsys-LS-D-LS-S" class="headerlink" title="fs_gsys (LS_D / LS_S)"></a><code>fs_gsys (LS_D / LS_S)</code></h3><ul><li>Use Bubble Sort </li><li>LS_D:LS_D list all files name in the directory and order by modified time of files, least modified, first print. </li><li>LS_S: list all files name and size in the directory and order by descending size. If there are several files with the same size, then first create first print. </li></ul><p>‚Äî‚Äî‚Äî‚ÄîBonus‚Äî‚Äî‚Äî‚Äî‚Äî</p><h3 id="fs-open-fs-name-G-READ-G-WRITE-1"><a href="#fs-open-fs-name-G-READ-G-WRITE-1" class="headerlink" title="fs_open(fs, name, G_READ/G_WRITE)"></a><code>fs_open(fs, name, G_READ/G_WRITE)</code></h3><ul><li>update: when create a new file, modify the current directory‚Äôs <code>file_size</code> and <code>modified_time</code>;</li><li>update: when remove a file, modify the current directory‚Äôs <code>file_size</code>;</li></ul><h3 id="fs-gsys-fs-MKDIR-name"><a href="#fs-gsys-fs-MKDIR-name" class="headerlink" title="fs_gsys(fs, MKDIR, name)"></a><code>fs_gsys(fs, MKDIR, name)</code></h3><ul><li>Similar with create an empty file in <code>fs_open</code></li></ul><h3 id="fs-gsys-fs-CD-name"><a href="#fs-gsys-fs-CD-name" class="headerlink" title="fs_gsys(fs,CD,name)"></a><code>fs_gsys(fs,CD,name)</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fp = linear search to get FCB index by name;</span><br><span class="line">fs-&gt;curr_dir = fp; <span class="comment">// update the current directory;</span></span><br></pre></td></tr></table></figure><h3 id="fs-gsys-fs-CD-P"><a href="#fs-gsys-fs-CD-P" class="headerlink" title="fs_gsys(fs,CD_P)"></a><code>fs_gsys(fs,CD_P)</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// change to the parent directory of the current directory;</span></span><br><span class="line">fs-&gt;curr_dir = fs-&gt;FCB_arr[fs-&gt;curr_dir].parent;</span><br></pre></td></tr></table></figure><h3 id="fs-gsys-fs-RM-RF-name"><a href="#fs-gsys-fs-RM-RF-name" class="headerlink" title="fs_gsys(fs,RM_RF,name)"></a><code>fs_gsys(fs,RM_RF,name)</code></h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// recursive_remove_dir(fs, fp);</span></span><br><span class="line"><span class="keyword">if</span> (directory fp is empty) remove the empty directory fp;</span><br><span class="line"><span class="keyword">for</span> (i in range of total files number) &#123;</span><br><span class="line">    <span class="keyword">if</span> file i is a file &#123;</span><br><span class="line">        remove file i;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (file i is a directory) &#123;</span><br><span class="line">        <span class="built_in">recursive_remove_dir</span>(fs, i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">remove empty directory fp;</span><br></pre></td></tr></table></figure><h3 id="fs-gsys-fs-PWD"><a href="#fs-gsys-fs-PWD" class="headerlink" title="fs_gsys(fs, PWD)"></a><code>fs_gsys(fs, PWD)</code></h3><ul><li>Back track the parent directory until reach to the root directory. Print out the current path.</li></ul><h3 id="fs-gsy-fs-LS-D-LS-S"><a href="#fs-gsy-fs-LS-D-LS-S" class="headerlink" title="fs_gsy(fs, LS_D\LS_S)"></a><code>fs_gsy(fs, LS_D\LS_S)</code></h3><h2 id="update-list-the-files-as-well-as-directories-For-a-file-list-it-name-with-size-only-For-a-directory-add-an-symbol-‚Äòd‚Äô-at-the-end"><a href="#update-list-the-files-as-well-as-directories-For-a-file-list-it-name-with-size-only-For-a-directory-add-an-symbol-‚Äòd‚Äô-at-the-end" class="headerlink" title="   * update: list the files as well as directories. For a file, list it name (with size) only. For a directory, add an symbol ‚Äòd‚Äô at the end."></a>   * update: list the files as well as directories. For a file, list it name (with size) only. For a directory, add an symbol ‚Äòd‚Äô at the end.</h2><h1 id="Project-5-Control-Device-in-Kernel-Module"><a href="#Project-5-Control-Device-in-Kernel-Module" class="headerlink" title="Project 5: Control Device in Kernel Module"></a>Project 5: Control Device in Kernel Module</h1><img src="/2022/03/25/CSC3150-Operating-System/dma.png" class="">]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Project-1-User-Thread-and-Kernel-Thread&quot;&gt;&lt;a href=&quot;#Project-1-User-Thread-and-Kernel-Thread&quot; class=&quot;headerlink&quot; title=&quot;Project 1: Use</summary>
      
    
    
    
    <category term="course" scheme="https://luuvy757.github.io/categories/course/"/>
    
    
  </entry>
  
  <entry>
    <title>CSC3050 Computer Architecture</title>
    <link href="https://luuvy757.github.io/2022/03/22/CSC3050-Computer-Architecture/"/>
    <id>https://luuvy757.github.io/2022/03/22/CSC3050-Computer-Architecture/</id>
    <published>2022-03-22T05:59:55.000Z</published>
    <updated>2023-02-23T02:58:00.781Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Project-1-MIPS-assembler-simulator"><a href="#Project-1-MIPS-assembler-simulator" class="headerlink" title="Project 1: MIPS: assembler, simulator."></a>Project 1: MIPS: assembler, simulator.</h1><blockquote><p>Assembler: translate MIPS code to machine code. </p><p>Simulator: execute machine code.</p></blockquote><h2 id="Generic-method"><a href="#Generic-method" class="headerlink" title="Generic method"></a>Generic method</h2><ol><li>Read .asm data and put data into static data segment</li><li>Read .asm text and use assembler to translate them into machine code, then put them into text segment</li><li>Initialize <code>pc = 0x40000000</code>, fetch instructions in text segment which incrementing <code>pc</code>.</li></ol><h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem:"></a>Problem:</h2><ol><li>How to put data/text into memory?<br> A: map virtual memory to real memory. </li><li>How to put data with different data type (char, byte, word, half)?<br> A: mapMem -&gt; <em>char, cast `</em>char <code>into</code><em>uint_16_t (half) </em>uint_32_t(word)`. </li><li>How to map <code>labels</code>?<br> A: when reading <code>.asm</code> file, using map data structure to map <code>label</code> and corresponding relative address.</li><li>Jump/Branch?<br>A: Jump <code>j label/address</code>, if label, label-&gt;relative address &gt;&gt; 2, then PC || address||00;<br>Branch <code>bne $s1 $s0 label/offset</code>, label -&gt; relative address =  pc+offset*4 + 4 =&gt; offset; then branch to PC + offset*4 + 4;</li></ol><h1 id="Project-2-ALU-Verilog"><a href="#Project-2-ALU-Verilog" class="headerlink" title="Project 2: ALU Verilog"></a>Project 2: ALU Verilog</h1><blockquote><p>ALU(regA, regB, instruction\control, result, flag(Z,O,N))</p></blockquote><h2 id="Generic-method-1"><a href="#Generic-method-1" class="headerlink" title="Generic method"></a>Generic method</h2><ul><li>Instruction decode =&gt; op code (R type always 0), func code</li><li>ALU module (<strong>always block</strong> with sensitive list)</li><li>Test bench (<strong>initial block</strong> for simulation purpose, delay 10ns for propagation delay)</li><li><code>always block</code> and <code>initial block</code> are independent processes </li></ul><h1 id="Project-3-Pipelined-CPU-with-Verilog"><a href="#Project-3-Pipelined-CPU-with-Verilog" class="headerlink" title="Project 3: Pipelined CPU with Verilog"></a>Project 3: Pipelined CPU with Verilog</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><img src="/2022/03/22/CSC3050-Computer-Architecture/datapath_diagram.jpg" class=""><img src="/2022/03/22/CSC3050-Computer-Architecture/mainController.jpg" class=""><h2 id="Generic-method-2"><a href="#Generic-method-2" class="headerlink" title="Generic method"></a>Generic method</h2><h3 id="Data-Path"><a href="#Data-Path" class="headerlink" title="Data Path"></a>Data Path</h3><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">PipelinednDatapath</span></span><br><span class="line"><span class="comment">IF -&gt; ID -&gt; EXE -&gt; MEM -&gt; WB</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">module</span> datapath#(</span><br><span class="line"><span class="keyword">parameter</span> DATA_WIDTH = <span class="number">32</span>,</span><br><span class="line"><span class="keyword">parameter</span> ADDR_WIDTH = <span class="number">5</span></span><br><span class="line">)(</span><br><span class="line"><span class="keyword">input</span> clk,</span><br><span class="line"><span class="keyword">input</span> rst,</span><br><span class="line"><span class="keyword">input</span> en_n</span><br><span class="line">);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Main-Controller"><a href="#Main-Controller" class="headerlink" title="Main Controller"></a>Main Controller</h3><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> controlUnit (</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">5</span>:<span class="number">0</span>] op,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">5</span>:<span class="number">0</span>] funct,</span><br><span class="line">    <span class="keyword">output</span>  Jump,</span><br><span class="line">    <span class="keyword">output</span>  RegWrite,  <span class="comment">// control register file -- 1 for write;</span></span><br><span class="line">    <span class="keyword">output</span>  MemtoReg,  <span class="comment">// 1 for data from mem, 0 for data from ALU output</span></span><br><span class="line">    <span class="keyword">output</span>  MemRead,  <span class="comment">// control data memory</span></span><br><span class="line">    <span class="comment">// output  MemWrite,  // control data memory</span></span><br><span class="line">    <span class="keyword">output</span>  BranchBNE,  <span class="comment">// for bne and beq;</span></span><br><span class="line">    <span class="keyword">output</span>  BranchBEQ,</span><br><span class="line">    <span class="keyword">output</span>  ALUSrc, <span class="comment">// 0 for regInput, 1 for immediate input</span></span><br><span class="line">    <span class="keyword">output</span>  [<span class="number">1</span>:<span class="number">0</span>] RegDst,  <span class="comment">// 0 for rt, 1 for rd (R type), 2 for reg31 (jal)</span></span><br><span class="line">    <span class="keyword">output</span>  isSigned,</span><br><span class="line">    <span class="keyword">output</span>  JumpRegID,  <span class="comment">// for jr: jump to address stored in a register</span></span><br><span class="line">    <span class="keyword">output</span>  RegWriteSrc, <span class="comment">// 0: data from DMEM; 1: PC + 4;</span></span><br><span class="line">    <span class="keyword">output</span>  [<span class="number">2</span>:<span class="number">0</span>] ALUop  <span class="comment">// input to ALU control unit</span></span><br><span class="line"></span><br><span class="line">);</span><br></pre></td></tr></table></figure><h3 id="ALU-controller"><a href="#ALU-controller" class="headerlink" title="ALU controller"></a>ALU controller</h3><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> ALUcontrol (</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">2</span>:<span class="number">0</span>] ALUop,  <span class="comment">// from the main controller</span></span><br><span class="line">    <span class="keyword">input</span> [<span class="number">5</span>:<span class="number">0</span>] funct,</span><br><span class="line">    <span class="keyword">output</span> [<span class="number">3</span>:<span class="number">0</span>] control,</span><br><span class="line"><span class="keyword">output</span> jump_signal</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h3 id="ALU"><a href="#ALU" class="headerlink" title="ALU"></a>ALU</h3><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> alu #(</span><br><span class="line">    <span class="keyword">parameter</span> DELAY = <span class="number">0</span></span><br><span class="line">)(</span><br><span class="line">    <span class="keyword">input</span> clk,</span><br><span class="line">    <span class="keyword">input</span> rst,</span><br><span class="line">    <span class="keyword">input</span> enable,  <span class="comment">// for delay enable, active high</span></span><br><span class="line">    <span class="keyword">input</span> [<span class="number">3</span>:<span class="number">0</span>] control,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">31</span>:<span class="number">0</span>] A,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">31</span>:<span class="number">0</span>] B,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">4</span>:<span class="number">0</span>] shamt,</span><br><span class="line">    <span class="keyword">output</span> [<span class="number">31</span>: <span class="number">0</span>] result,</span><br><span class="line">    <span class="keyword">output</span> Zero,</span><br><span class="line">    <span class="keyword">output</span> Overflow,</span><br><span class="line">    <span class="keyword">output</span> Signbit,</span><br><span class="line">    <span class="keyword">output</span> Carrybit</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h3 id="Register-File"><a href="#Register-File" class="headerlink" title="Register File"></a>Register File</h3><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> RegisterFile (</span><br><span class="line">    <span class="keyword">input</span> clk,</span><br><span class="line">    <span class="keyword">input</span> rst,</span><br><span class="line">    <span class="keyword">input</span> RegWrite,  <span class="comment">// register write signal</span></span><br><span class="line">    <span class="keyword">input</span> [<span class="number">4</span>:<span class="number">0</span>] readReg1,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">4</span>:<span class="number">0</span>] readReg2,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">4</span>:<span class="number">0</span>] writeReg,   <span class="comment">// indicating which the register to write to.</span></span><br><span class="line">    <span class="keyword">input</span> [<span class="number">31</span>:<span class="number">0</span>] writeData,  </span><br><span class="line">    <span class="keyword">output</span> [<span class="number">31</span>:<span class="number">0</span>] readData1,</span><br><span class="line">    <span class="keyword">output</span> [<span class="number">31</span>:<span class="number">0</span>] readData2</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h3 id="Data-Memory"><a href="#Data-Memory" class="headerlink" title="Data Memory"></a>Data Memory</h3><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> MainMemory</span><br><span class="line">    ( <span class="comment">// Inputs</span></span><br><span class="line">      <span class="keyword">input</span>  CLOCK <span class="comment">// clock</span></span><br><span class="line">    , <span class="keyword">input</span>  RESET <span class="comment">// reset</span></span><br><span class="line">    , <span class="keyword">input</span>  MEMWRITE <span class="comment">// write signal</span></span><br><span class="line">    , <span class="keyword">input</span>  MEMREAD <span class="comment">// read signal</span></span><br><span class="line">    , <span class="keyword">input</span> [<span class="number">31</span>:<span class="number">0</span>] ADDRESS</span><br><span class="line">    , <span class="keyword">input</span> [<span class="number">31</span>:<span class="number">0</span>] WRITE_DATA <span class="comment">// write data</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// Outputs</span></span><br><span class="line">    , <span class="keyword">output</span> [<span class="number">31</span>:<span class="number">0</span>] READDATA  <span class="comment">// output readData</span></span><br><span class="line">    );</span><br></pre></td></tr></table></figure><h3 id="sign-extender"><a href="#sign-extender" class="headerlink" title="sign extender"></a>sign extender</h3><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> signextender #(</span><br><span class="line"><span class="keyword">parameter</span> DELAY = <span class="number">0</span></span><br><span class="line">)(</span><br><span class="line"><span class="keyword">input</span> [<span class="number">15</span>:<span class="number">0</span>] dataIn,</span><br><span class="line"><span class="keyword">output</span> [<span class="number">31</span>:<span class="number">0</span>] dataOut,</span><br><span class="line"><span class="keyword">input</span> isSigned,  <span class="comment">// extend 1 or 0;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Delay Inputs</span></span><br><span class="line"><span class="keyword">input</span> clk,</span><br><span class="line"><span class="keyword">input</span> rst,</span><br><span class="line"><span class="keyword">input</span> en</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h3 id="mux"><a href="#mux" class="headerlink" title="mux"></a>mux</h3><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** ---MUX---- **/</span></span><br><span class="line"><span class="comment">// n inputs to 1 output</span></span><br><span class="line"><span class="comment">// the inputs is depended on the select signal</span></span><br><span class="line"><span class="keyword">module</span> MUX #(</span><br><span class="line"><span class="keyword">parameter</span> DATA_INPUT_WIDTH = <span class="number">5</span>,  <span class="comment">// the input data width</span></span><br><span class="line"><span class="keyword">parameter</span> DATA_INPUT_NUM= <span class="number">2</span>,  <span class="comment">// the number of input data</span></span><br><span class="line"><span class="keyword">parameter</span> SEL_BIT_WIDTH = <span class="number">1</span>  <span class="comment">// the input signal data width, if there is n input datas, the select input data width is log2(n) bits.</span></span><br><span class="line">)(</span><br><span class="line"><span class="keyword">input</span> [DATA_INPUT_WIDTH*DATA_INPUT_NUM- <span class="number">1</span>:<span class="number">0</span>] dataIn,  <span class="comment">// n inputs packed into an array</span></span><br><span class="line"><span class="keyword">input</span> [SEL_BIT_WIDTH - <span class="number">1</span>:<span class="number">0</span>] sel,  </span><br><span class="line"><span class="keyword">output</span> [DATA_INPUT_WIDTH - <span class="number">1</span>:<span class="number">0</span>] dataOut</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h3 id="pipe-register"><a href="#pipe-register" class="headerlink" title="pipe register"></a>pipe register</h3><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* delay</span></span><br><span class="line"><span class="comment"> * This creates an array or vector registers of generic bitwidth, array width, and pipeline depth. </span></span><br><span class="line"><span class="comment"> *  Pipe shifts makes delay</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> delay #(</span><br><span class="line"><span class="keyword">parameter</span> BIT_WIDTH = <span class="number">32</span>,</span><br><span class="line"><span class="keyword">parameter</span> DEPTH = <span class="number">2</span>,  <span class="comment">// the number of input/output data</span></span><br><span class="line"><span class="keyword">parameter</span> DELAY = <span class="number">4</span>  <span class="comment">// pipeline depth </span></span><br><span class="line">)(</span><br><span class="line"><span class="keyword">input</span> clk,</span><br><span class="line"><span class="keyword">input</span> rst, <span class="comment">// synchronous reset, active high</span></span><br><span class="line"><span class="keyword">input</span> en,  </span><br><span class="line"><span class="keyword">input</span> [BIT_WIDTH*DEPTH - <span class="number">1</span>:<span class="number">0</span>] dataIn,  <span class="comment">//input data that packed into one array </span></span><br><span class="line"><span class="keyword">output</span> [BIT_WIDTH*DEPTH - <span class="number">1</span>:<span class="number">0</span>] dataOut  <span class="comment">// output data that packed into one array.</span></span><br><span class="line"> </span><br><span class="line">);</span><br></pre></td></tr></table></figure><h3 id="Hazard-Detection-Unit"><a href="#Hazard-Detection-Unit" class="headerlink" title="Hazard Detection Unit"></a>Hazard Detection Unit</h3><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">USAGE: </span></span><br><span class="line"><span class="comment">1. lw data hazard; </span></span><br><span class="line"><span class="comment">2. Branch Data Hazard</span></span><br><span class="line"><span class="comment">3. Branch Hazard (j, jr, jal, bne, beq),</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">module</span> hazardDetectionUnit (</span><br><span class="line"><span class="keyword">input</span> [<span class="number">31</span>:<span class="number">0</span>] IF_ID_Instruction,</span><br><span class="line"><span class="keyword">input</span> ID_EX_MemRead,</span><br><span class="line"><span class="keyword">input</span> [<span class="number">4</span>:<span class="number">0</span>] ID_EX_Rt,</span><br><span class="line"><span class="keyword">input</span> equal,</span><br><span class="line"><span class="keyword">input</span> [<span class="number">4</span>:<span class="number">0</span>] ID_EX_Rd,</span><br><span class="line"><span class="keyword">input</span> [<span class="number">4</span>:<span class="number">0</span>] EX_MEM_Rd,</span><br><span class="line"><span class="keyword">input</span> ID_EX_RegWrite,</span><br><span class="line"><span class="keyword">input</span> EX_MEM_RegWrite,</span><br><span class="line"><span class="keyword">output</span> <span class="keyword">reg</span> PCWrite,</span><br><span class="line"><span class="keyword">output</span> <span class="keyword">reg</span> ID_EX_Flush,</span><br><span class="line"><span class="keyword">output</span> <span class="keyword">reg</span> IF_Flush,</span><br><span class="line"><span class="keyword">output</span> <span class="keyword">reg</span> IF_ID_Hold</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h2 id="Hazard"><a href="#Hazard" class="headerlink" title="Hazard"></a>Hazard</h2><h3 id="Structural-Memory-hazard"><a href="#Structural-Memory-hazard" class="headerlink" title="Structural(Memory) hazard:"></a><strong>Structural(Memory) hazard</strong>:</h3><p>separate Instruction Memory and Data Memory</p><h3 id="Data-Hazard-only-RAW-can-happen-in-MIPS"><a href="#Data-Hazard-only-RAW-can-happen-in-MIPS" class="headerlink" title="Data Hazard: only RAW can happen in MIPS"></a><strong>Data Hazard</strong>: only RAW can happen in MIPS</h3><h4 id="insert-nops-software"><a href="#insert-nops-software" class="headerlink" title="insert nops (software);"></a>insert nops (software);</h4><h4 id="Forwarding-R-type-hardware"><a href="#Forwarding-R-type-hardware" class="headerlink" title="Forwarding (R_type, hardware);"></a>Forwarding (R_type, hardware);</h4><img src="/2022/03/22/CSC3050-Computer-Architecture/forwarding.png" class=""><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">Forwarding Unit</span></span><br><span class="line"><span class="comment">Mux control | Source | Explanation</span></span><br><span class="line"><span class="comment">ForwardA = 00   | ID/EX  | The first ALU operand comes from the register file</span></span><br><span class="line"><span class="comment">ForwardA = 10   | EX/MEM | The first ALU operand is forwarded from the prior ALU result</span></span><br><span class="line"><span class="comment">ForwardA = 01   | MEM/WB | The first ALU operand is forwarded from data memory or an earlier ALU result</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">ForwardB = 00   | ID/EX  | The second ALU operand comes from the register file</span></span><br><span class="line"><span class="comment">ForwardB = 01   | EX/MEM | The second ALU operand is forwarded from the prior ALU result</span></span><br><span class="line"><span class="comment">ForwardB = 10   | MEM/WB | The second ALU operand is forwarded from data memory or an earlier ALU result</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">Two optimizations</span></span><br><span class="line"><span class="comment">‚Äì Do not forward if instruction does not write register</span></span><br><span class="line"><span class="comment">=&gt; check if RegWrite is asserted</span></span><br><span class="line"><span class="comment">‚Äì Do not forward if destination register is $0</span></span><br><span class="line"><span class="comment">=&gt; check if RegisterRd = 0</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">module</span> forwardingUnit(</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">4</span>:<span class="number">0</span>] ID_EX_Rs,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">4</span>:<span class="number">0</span>] ID_EX_Rt,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">4</span>:<span class="number">0</span>] EX_MEM_Rd,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">4</span>:<span class="number">0</span>] MEM_WB_Rd,</span><br><span class="line">    <span class="keyword">input</span> EX_MEM_RegWrite,</span><br><span class="line">    <span class="keyword">input</span> MEM_WB_RegWrite,</span><br><span class="line">    <span class="keyword">output</span> [<span class="number">1</span>:<span class="number">0</span>] ForwardA,</span><br><span class="line">    <span class="keyword">output</span> [<span class="number">1</span>:<span class="number">0</span>] ForwardB</span><br><span class="line">);</span><br></pre></td></tr></table></figure><img src="/2022/03/22/CSC3050-Computer-Architecture/detect_data_hazard.png" class=""><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> hazardDetectionUnit (</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">31</span>:<span class="number">0</span>] IF_ID_Instruction,</span><br><span class="line">    <span class="keyword">input</span> ID_EX_MemRead,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">4</span>:<span class="number">0</span>] ID_EX_Rt,</span><br><span class="line">    <span class="keyword">input</span> equal,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">4</span>:<span class="number">0</span>] ID_EX_Rd,</span><br><span class="line">    <span class="keyword">input</span> [<span class="number">4</span>:<span class="number">0</span>] EX_MEM_Rd,</span><br><span class="line">    <span class="keyword">input</span> ID_EX_RegWrite,</span><br><span class="line">    <span class="keyword">input</span> EX_MEM_RegWrite,</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> PCWrite,</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> ID_EX_Flush,</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> IF_Flush,</span><br><span class="line">    <span class="keyword">output</span> <span class="keyword">reg</span> IF_ID_Hold</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h4 id="stalls-load-use-hardware"><a href="#stalls-load-use-hardware" class="headerlink" title="stalls (load-use, hardware)"></a>stalls (load-use, hardware)</h4><img src="/2022/03/22/CSC3050-Computer-Architecture/stalling.png" class=""><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if (ID/EX.MemRead and ((ID/EX.RegisterRt = IF/ID.RegisterRs) or</span><br><span class="line">        (ID/EX.RegisterRt = IF/ID.registerRt)) )  stall the pipeline for one cycle</span><br><span class="line">/* ID/EX.MemRead=1 indicates a load instruction */ </span><br></pre></td></tr></table></figure><ul><li><p>How to stall?</p><p> ‚Äì Stall instruction in IF and ID: not change PC and IF/ID<br> =&gt; the stages re-execute the instructions<br> ‚Äì What to move into EX: insert an NOP by changing EX, MEM, WB control fields of ID/EX pipeline register to 0<br> ‚Ä¢ All control signals to EX, MEM, WB are deasserted and no registers or memories<br> are written</p></li></ul><h3 id="Branch-hazard"><a href="#Branch-hazard" class="headerlink" title="Branch hazard:"></a>Branch hazard:</h3><img src="/2022/03/22/CSC3050-Computer-Architecture/branch_hazard.png" class="">    <p>Assume branch not taken<br>‚Äì Need to add hardware for flushing instruction if wrong<br>‚Äì Branch decision made at MEM =&gt; need to flush instruction in IF/ID, ID/EX by changing control values to 0</p><p>The <code>mainController</code> output the <code>BranchBEQ</code> and <code>BranchBNE</code> signals. The<br><code>hazardDetectionUnit</code> is used to handle <strong>BRANCH Hazard</strong> and <strong>BRANCH DATA Hazard</strong>.</p><h2 id="How-register-file-simultaneously-writes-and-reads"><a href="#How-register-file-simultaneously-writes-and-reads" class="headerlink" title="How register file simultaneously writes and reads?"></a>How register file simultaneously writes and reads?</h2><p>The <code>RegisterFile</code> has a sensitive input clk, that is triggered by falling edge. The<br>reason is:<br>Read request of value written in the same cycle occurs in all kind of designs, and is<br>usually handled with bypass logic, whereby the written value is forwarded directly to<br>the read output, without going through the registers. Such bypass is done in a single<br>clock design.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Project-1-MIPS-assembler-simulator&quot;&gt;&lt;a href=&quot;#Project-1-MIPS-assembler-simulator&quot; class=&quot;headerlink&quot; title=&quot;Project 1: MIPS: assembl</summary>
      
    
    
    
    <category term="course" scheme="https://luuvy757.github.io/categories/course/"/>
    
    
  </entry>
  
  <entry>
    <title>FL Algorithms</title>
    <link href="https://luuvy757.github.io/2022/03/09/FL-Algorithms/"/>
    <id>https://luuvy757.github.io/2022/03/09/FL-Algorithms/</id>
    <published>2022-03-09T14:03:03.000Z</published>
    <updated>2023-02-23T02:57:56.089Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Device $i$ has $N<em>{i}$ training samples: $z</em>{i,1}, z<em>{i,2}, ‚Ä¶, z</em>{i,N_{i}}$   </li><li>$f<em>{i}(w, z</em>{i,j})$: loss of the model on training sample $z_{i,j}$     </li><li>$F<em>{i}(w) = \frac{1}{N</em>{i}} \sum<em>{j=1}^{N</em>{i}}f<em>{i}(w, z</em>{i,j})$: loss function of device $i$    </li></ul><h2 id="Formula-1-One-Model-Fits-All-Formulation"><a href="#Formula-1-One-Model-Fits-All-Formulation" class="headerlink" title="Formula 1: One-Model-Fits-All Formulation"></a>Formula 1: One-Model-Fits-All Formulation</h2><script type="math/tex; mode=display">minimize_{w} \sum_{i=1}^{n}\alpha_{i}F_{i}(w)</script><p>Common practice: $\alpha<em>{i} = \frac{N</em>{i}}{N}, N=\sum<em>{k=1}^{n} N</em>{k}$:</p><script type="math/tex; mode=display">minimize_{w}\frac{1}{N}\sum_{i=1}^{n}\sum_{j=1}^{N_{i}}f_{i}(w, z_{i,j})</script><p>(Less effective and even undesirable)</p><h2 id="Formula-2-Full-personalized-model"><a href="#Formula-2-Full-personalized-model" class="headerlink" title="Formula 2: Full personalized model"></a>Formula 2: Full personalized model</h2><script type="math/tex; mode=display">minimize_{w_{0}, \{w_{i}\}_{i=1}^{n}} \sum_{i=1}^{n}\alpha_{i}(F_{i}(w_{i} + \frac{\lambda_{i}}{2}||w_{i}-w_{0}||^{2}))</script><ul><li>$w_{i}$: personalized model parameters.</li><li>$\lambda_{i}$: regularization weights.</li><li>$w_{0}$: reference model maintained by server.</li></ul><p>(Full model personalization: $w<em>{i}, w</em>{0}$, <strong>memory cost</strong>)</p><h2 id="Formula-3-Partial-personalized-model"><a href="#Formula-3-Partial-personalized-model" class="headerlink" title="Formula 3: Partial personalized model"></a>Formula 3: Partial personalized model</h2><ul><li>model parameters on device $i$: $w<em>{i} = (u,v</em>{i})$</li><li>$u$: shared parameters $u \in R^{d_{0}}$</li><li>$v<em>{i}$: personalized parameters $v</em>{i} \in R^{d_{i}}$</li></ul><script type="math/tex; mode=display">minimize_{u, \{v_{i}\}_{i=1}^{n}}\sum_{i=1}^{n}\alpha_{i}F_{i}(u,v_{i})</script><p><strong>Problems</strong>: different  of</p><ul><li>$\dim v_{i}$ </li><li>Number of parameters </li><li>Architecture </li></ul><p><strong>Solution</strong>: <em>FedSim, FedAlt</em></p><h2 id="Standard-FL-protocol"><a href="#Standard-FL-protocol" class="headerlink" title="Standard FL protocol:"></a>Standard FL protocol:</h2><img src="/2022/03/09/FL-Algorithms/FL_standard.png" class=""><ol><li>During each round, the <strong>server</strong> <em>randomly</em> selects a subset of the devices for <strong>update and broadcasts</strong> the current global version of the <strong>shared parameters</strong> to devices in the subset. </li><li>Each <strong>selected device</strong> then<br>performs one or more steps of (stochastic) gradient descent to <strong>update both the shared parameters and the personal parameters</strong>, and <strong>sends the updated shared parameters to the server</strong> for aggregation.</li><li>The <strong>updated personal parameters</strong> are kept local at the device to serve as the initial states when the device is selected for another update. <h2 id="FedSim-FedAlt"><a href="#FedSim-FedAlt" class="headerlink" title="FedSim, FedAlt"></a><em>FedSim, FedAlt</em></h2><img src="/2022/03/09/FL-Algorithms/FedSim_FedAlt.png" class=""></li></ol><ul><li><p>In <em>FedSim</em>, the <strong>shared and personal parameters</strong> are <strong>updated simultaneously</strong> during each local iteration.</p></li><li><p>In <em>FedAlt</em>, the devices <strong>first update the personal parameters</strong> with the received shared parameters fixed and <strong>then update the shared parameters</strong> with the new personal parameters fixed.</p></li></ul><h2 id="FedAvg"><a href="#FedAvg" class="headerlink" title="FedAvg"></a><em>FedAvg</em></h2><img src="/2022/03/09/FL-Algorithms/FedAvg_Algorithm.png" class=""><blockquote><p>Add Computation to decrease communication     </p><p><strong>Computation controlled by</strong></p><ul><li>C: fraction of clients that perform computation on each round</li><li>E: local epochs</li><li>B: local minibatch size. $\infty$: full local dataset is a single batch.</li></ul></blockquote><p>Client $k$ with $n<em>{k}$ local samples, local updates per round is $u</em>{k} = E \frac{n_{k}}{B}$</p><p>Adding Local Computation: decrease B or increase E, or both.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;Device $i$ has $N&lt;em&gt;{i}$ training samples: $z&lt;/em&gt;{i,1}, z&lt;em&gt;{i,2}, ‚Ä¶, z&lt;/em&gt;{i,N_{i}}$   &lt;/li&gt;
&lt;li&gt;$f&lt;em&gt;{i}(w, z&lt;/em&gt;{i,j})$: l</summary>
      
    
    
    
    <category term="research" scheme="https://luuvy757.github.io/categories/research/"/>
    
    
    <category term="FL" scheme="https://luuvy757.github.io/tags/FL/"/>
    
  </entry>
  
  <entry>
    <title>FL using Flower on device</title>
    <link href="https://luuvy757.github.io/2022/01/21/FL-using-Flower-on-device/"/>
    <id>https://luuvy757.github.io/2022/01/21/FL-using-Flower-on-device/</id>
    <published>2022-01-21T04:59:06.000Z</published>
    <updated>2023-02-23T02:04:12.999Z</updated>
    
    <content type="html"><![CDATA[<h2 id=""><a href="#" class="headerlink" title=""></a><a href="/2022/01/23/FL-using-Flower-on-device/About-Flower/" title="About Flower">About Flower</a></h2><h2 id="Flower-Android"><a href="#Flower-Android" class="headerlink" title="Flower + Android"></a>Flower + Android</h2><img src="/2022/01/21/FL-using-Flower-on-device/flwr-android.png" class="" title="Overview"><h3 id="Run-Federated-Learning-on-Android-Devices-TF-Lite-Flower"><a href="#Run-Federated-Learning-on-Android-Devices-TF-Lite-Flower" class="headerlink" title="Run Federated Learning on Android Devices (TF Lite + Flower)"></a>Run Federated Learning on Android Devices (TF Lite + Flower)</h3><p><a href="https://flower.dev/blog/2021-12-15-federated-learning-on-android-devices-with-flower">Tutorial blog</a><br><a href="https://github.com/adap/flower/tree/main/examples/android">Github: Flower Android Example (TensorFlowLite)</a></p><ol><li><p><strong>Setup the model definitions</strong></p><blockquote><p>tflite_convertor/convert_to_tflite.py<br><a href="#FL-Model-Personalization">TF Model Personalization</a> requires defining two architectures:      </p></blockquote><ul><li><strong>Base Model</strong> <ul><li>A pre-trained feature extractor (e.g., ResNet50 trained on ImageNet) which is not updated during on-device training.</li></ul></li><li><p><strong>Head Model</strong> </p><ul><li>Like a task-specific classifer which is randomly initialized and trained on the local data.</li></ul></li><li><p>Both Models can be configured in <strong>tflite_convertor/convert_to_tflite.py</strong>, and converted into ‚Äútflite_model‚Äù</p></li></ul></li></ol><ol><li><p><strong>Android Client</strong> </p><blockquote><p><code>client</code> folder  </p></blockquote><ul><li><strong>Modifying App‚Äôs Interface and Functionalities</strong><br>flwr/android_client/MainActivity.java<br><code>loadData</code>, <code>connect</code>, <code>runGRCP</code></li><li><p><strong>Modifying Flower Client</strong><br>flwr/android_client/FlowerClient.java<br><strong>LoadData</strong> (Also, checking: src/main/assets/data,<br>  flwr/android_client/TransferLearningModelWrapper.java)<br><strong>Loss Callback</strong>          </p></li><li><p><strong>Add tflite_model</strong><br>src/main/assets/model<br>build.gradle(Module: client.app)</p></li></ul></li><li><p><strong>Server</strong></p><blockquote><p>server.py</p></blockquote><p>Usage: <code>Strategy</code> configuration</p><p>More about Strategy: <a href="https://flower.dev/docs/strategies.html">https://flower.dev/docs/strategies.html</a></p></li><li><p>Log</p><ul><li><p>Server Log</p><img src="/2022/01/21/FL-using-Flower-on-device/demo-server-log.png" class=""></li><li><p>Client Log (Two devices: Huawei Honor (Android 9) and Huawei M6 (Android 10))</p><img src="/2022/01/21/FL-using-Flower-on-device/demo-device-log.jpg" class=""></li></ul></li></ol><h2 id="Flower-Embedded"><a href="#Flower-Embedded" class="headerlink" title="Flower + Embedded"></a>Flower + Embedded</h2><img src="/2022/01/21/FL-using-Flower-on-device/flwr-embedded.png" class="" title="Overview"><hr><h2 id="FL-Model-Personalization"><a href="#FL-Model-Personalization" class="headerlink" title="FL Model Personalization"></a>FL Model Personalization</h2><p><a href="https://github.com/tensorflow/examples/tree/master/lite/examples/model_personalization">TensorFlow Lite Example On-device Model Personalization</a></p><h3 id="Prepare-the-TfLite-model"><a href="#Prepare-the-TfLite-model" class="headerlink" title="Prepare the TfLite model"></a>Prepare the TfLite model</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/model_personalization</span><br><span class="line">.</span><br><span class="line">‚îú‚îÄ‚îÄ README.md</span><br><span class="line">‚îú‚îÄ‚îÄ android</span><br><span class="line">‚îú‚îÄ‚îÄ app_screenshot.png</span><br><span class="line">‚îî‚îÄ‚îÄ transfer_learning</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># under /transfer_learning </span></span><br><span class="line"><span class="comment"># Generate the model flatbuffer file `model.tflite`</span></span><br><span class="line">python generate_training_model.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># under /model_personalization</span></span><br><span class="line"><span class="comment"># Copy over the flatbuffer file to the `android` assets directory.</span></span><br><span class="line">cp transfer_learning/model.tflite android/app/src/main/assets/model/model.tflite</span><br></pre></td></tr></table></figure><h3 id="Install-and-run-the-application-on-Device"><a href="#Install-and-run-the-application-on-Device" class="headerlink" title="Install and run the application on Device"></a>Install and run the application on Device</h3><ol><li><p>Open Android Studio and import the project (the <code>/android</code> folder)</p><img src="/2022/01/21/FL-using-Flower-on-device/android-studio-import-demo.png" class=""></li><li><p>Connect with device and run the code. </p><img src="/2022/01/21/FL-using-Flower-on-device/device-connection.png" class=""></li></ol><h3 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h3><ol><li><p><code>$ python server.py</code> Error: No attribute FedAvgAndroid</p><blockquote><p>Install the latest version of the flower from the github:</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install git+https://github.com/adap/flower.git</span><br></pre></td></tr></table></figure></blockquote></li><li><p>Failed install app: app cannot be installed for ‚Äúunpublished vision‚Äù</p><blockquote><p><code>/android</code> folder, in <code>gradle.properties</code> add <code>android.injected.testOnly=false</code></p></blockquote></li><li><p>USB connection</p><blockquote><p>Open USB developer option on device</p></blockquote></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;&quot;&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot;&quot;&gt;&lt;/a&gt;&lt;a href=&quot;/2022/01/23/FL-using-Flower-on-device/About-Flower/&quot; title=&quot;About Flower&quot;&gt;Abo</summary>
      
    
    
    
    
    <category term="FL" scheme="https://luuvy757.github.io/tags/FL/"/>
    
  </entry>
  
</feed>
