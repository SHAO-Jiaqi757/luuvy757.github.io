<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>FL Survey | Jiaqi's Blog</title><meta name="keywords" content="literature"><meta name="author" content="Jiaqi Shao"><meta name="copyright" content="Jiaqi Shao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="SurveyFederated Machine Learning: Concept and Applications [Paper] ✔️Federated Learning: Challenges, Methods, and Future Directions [Paper] ✔️ Challenge 1: Expensive Communication. (i) reducing the to">
<meta property="og:type" content="article">
<meta property="og:title" content="FL Survey">
<meta property="og:url" content="https://luuvy757.github.io/2023/01/20/FL-Survey/index.html">
<meta property="og:site_name" content="Jiaqi&#39;s Blog">
<meta property="og:description" content="SurveyFederated Machine Learning: Concept and Applications [Paper] ✔️Federated Learning: Challenges, Methods, and Future Directions [Paper] ✔️ Challenge 1: Expensive Communication. (i) reducing the to">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://luuvy757.github.io/img/covers/20220310232325.png">
<meta property="article:published_time" content="2023-01-20T02:17:09.000Z">
<meta property="article:modified_time" content="2023-01-20T02:37:15.636Z">
<meta property="article:author" content="Jiaqi Shao">
<meta property="article:tag" content="literature">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://luuvy757.github.io/img/covers/20220310232325.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://luuvy757.github.io/2023/01/20/FL-Survey/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'FL Survey',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-01-20 10:37:15'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Jiaqi's Blog" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">7</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">2</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/covers/20220310232325.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Jiaqi's Blog</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">FL Survey</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-01-20T02:17:09.000Z" title="Created 2023-01-20 10:17:09">2023-01-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-01-20T02:37:15.636Z" title="Updated 2023-01-20 10:37:15">2023-01-20</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="FL Survey"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h1><h2 id="Federated-Machine-Learning-Concept-and-Applications-Paper-✔️"><a href="#Federated-Machine-Learning-Concept-and-Applications-Paper-✔️" class="headerlink" title="Federated Machine Learning: Concept and Applications [Paper] ✔️"></a><strong>Federated Machine Learning: Concept and Applications</strong> <a target="_blank" rel="noopener" href="https://dl.acm.org/citation.cfm?id=3298981" title="\[Paper]">[Paper]</a> ✔️</h2><h2 id="Federated-Learning-Challenges-Methods-and-Future-Directions-Paper-✔️"><a href="#Federated-Learning-Challenges-Methods-and-Future-Directions-Paper-✔️" class="headerlink" title="Federated Learning: Challenges, Methods, and Future Directions [Paper] ✔️"></a><strong>Federated Learning: Challenges, Methods, and Future Directions</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.07873" title="\[Paper]">[Paper]</a> ✔️</h2><ol>
<li><p><strong>Challenge 1: Expensive Communication.</strong> (i) reducing the total number of communication rounds, or (ii) reducing the <strong>size of transmitted messages</strong> at each round.</p>
<ol>
<li><p>Local Updating: reduce the total number of communication rounds</p>
<img src="/2023/01/20/FL-Survey/image_GMlwXt6aaQ.png" class=""></li>
<li>Compression Scheme: reduce the size of messages communicated at each round. (sparsification, subsampling, and quantization)</li>
<li><p>Decentralized Training</p>
<img src="/2023/01/20/FL-Survey/image_PeWLFhmo-E.png" class=""></li>
</ol>
</li>
<li><p><strong>Challenge 2: Systems Heterogeneity</strong>.variability in hardware (CPU, memory), network connectivity (3G, 4G, 5G, wifi), and power (battery level).(i) anticipate a low amount of participation, (ii) tolerate heterogeneous hardware, and (iii) be robust to dropped devices in the network.</p>
<ol>
<li><p>Asynchronous communication: mitigate <em>stragglers</em> in heterogeneous environments</p>
<img src="/2023/01/20/FL-Survey/image_LjfrgCZebd.png" class=""></li>
<li>Active device sampling: sampling policies based on systems resources, incentive mechanisms to encourage devices with higher-quality data.</li>
<li>Fault tolerance.</li>
</ol>
</li>
<li><p><strong>Challenge 3: Statistical Heterogeneity</strong>: non-iid data distribution, varying the number of data samples</p>
<ol>
<li><p>Modeling Heterogeneous Data</p>
<img src="/2023/01/20/FL-Survey/image_ZBNjxqb3K2.png" class=""></li>
<li>Convergence Guarantees for Non-IID Data</li>
</ol>
</li>
<li>Challenge 4: Privacy Concerns.<h2 id="Advances-and-Open-Problems-in-Federated-Learning-Paper"><a href="#Advances-and-Open-Problems-in-Federated-Learning-Paper" class="headerlink" title="Advances and Open Problems in Federated Learning [Paper]"></a><strong>Advances and Open Problems in Federated Learning</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.04977" title="\[Paper]">[Paper]</a></h2><h2 id="Federated-Learning-White-Paper-V1-0-Paper-✔️"><a href="#Federated-Learning-White-Paper-V1-0-Paper-✔️" class="headerlink" title="Federated Learning White Paper V1.0 [Paper] ✔️"></a>Federated Learning White Paper V1.0 <a target="_blank" rel="noopener" href="https://www.fedai.org/static/flwp-en.pdf" title="\[Paper]">[Paper]</a> ✔️</h2><h2 id="Federated-Learning-Systems-Vision-Hype-and-Reality-for-Data-Privacy-and-Protection-Paper-✔️"><a href="#Federated-Learning-Systems-Vision-Hype-and-Reality-for-Data-Privacy-and-Protection-Paper-✔️" class="headerlink" title="Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection [Paper] ✔️"></a>Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.09693" title="\[Paper]">[Paper]</a> ✔️</h2></li>
</ol>
<ul>
<li>Definition: Federated systems have different emphasis on collaboration and constraints. (1) FDBSs focus on the <strong>management</strong> of distributed data and (2) FCs [federated cloud] focus on the <strong>scheduling</strong> of the resources, (3) FLSs [Federated Learning System] care more about the secure computation among multiple parties. FLSs induce new challenges such as the algorithm designs of the distributed training and the data protection under the privacy restrictions.<ul>
<li>Parties:&#x20;<ul>
<li>the hardware capacity [e.g. the computation power and storage]&#x20;</li>
<li>the scale and stability: mobile devices are more scalable. Also, the stability of the cross-silo is better than the cross-device setting.&#x20;</li>
<li>the data distributions [e.g. non-iid]</li>
</ul>
</li>
<li>Manager: cross-device — a powerful central server; cross-silo — selected organization; (blockchain based)</li>
</ul>
</li>
<li>Evaluation: (1) model performance, (2) system security, (3) system efficiency, and (4) system robustness.<img src="/2023/01/20/FL-Survey/image_Cr8FuyVvkG.png" class="">
</li>
</ul>
<p><img src="image/image_aMFRwwuJpO.png" alt="Taxonomy of federated learning systems" title="Taxonomy of federated learning systems"></p>
<img src="/2023/01/20/FL-Survey/image_Yj5zT8RULN.png" class="">
<h2 id="Federated-Learning-in-Mobile-Edge-Networks-A-Comprehensive-Survey-Paper-✔️"><a href="#Federated-Learning-in-Mobile-Edge-Networks-A-Comprehensive-Survey-Paper-✔️" class="headerlink" title="Federated Learning in Mobile Edge Networks: A Comprehensive Survey [Paper] ✔️"></a>Federated Learning in Mobile Edge Networks: A Comprehensive Survey <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.11875" title="\[Paper]">[Paper]</a> ✔️</h2><img src="/2023/01/20/FL-Survey/image_20SuT40krc.png" class="">
<ul>
<li><p><strong>Communication Cost</strong></p>
<img src="/2023/01/20/FL-Survey/image_M0xWkQEWNe.png" class="">
<ul>
<li>Edge and end computation (decrease the number of communication rounds)<ul>
<li>increasing <strong>parallelism</strong>: <strong>more participants</strong> are selected to participate in each round of training</li>
<li>increasing computation per participant: each participant performs more <em>local updates</em> before communication for global aggregation.<br><img src="https://pdf.cdn.readpaper.com/parsed/fetch_target/739e63e50300d71de36e2952d6cc768f_9_Figure_6.png" alt="Approaches to increase computation at edge and end devices include (a) Increased computation at end devices, e.g., more passes over dataset before communication(b) Two-stream training with global model as a reference and (c) Intermediate edge server aggregation." title="Approaches to increase computation at edge and end devices include (a) Increased computation at end devices, e.g., more passes over dataset before communication(b) Two-stream training with global model as a reference and (c) Intermediate edge server aggregation."></li>
</ul>
</li>
<li>Model compression<ul>
<li>structured updates: restrict participant updates to have a pre-specific structure. [<em>low rank</em> \<only optimized matrix needs to be sent> and <em>random mask</em> \<only non-zero entries needs to be sent>]</li>
<li>sketched updates:  encoding the update in a compressed form before communication [subsampling approach, probabilistic qantization approach]<br><img src="image/image_Tu2xb7Ir1n.png" alt="The compression techniques considered are summarized above by the diagram from authors in \[99\]. (i) Federated dropout to reduce size of model (ii) Lossy compression of model (iii) Decompression for training (iv) Compression of participant updates (v) Decompression (vi) Global aggregation" title="The compression techniques considered are summarized above by the diagram from authors in \[99]. (i) Federated dropout to reduce size of model (ii) Lossy compression of model (iii) Decompression for training (iv) Compression of participant updates (v) Decompression (vi) Global aggregation"></li>
</ul>
</li>
<li>Important-based updating<ul>
<li>edge Stochastic Gradient Descent (eSGD) algorithm: selects only a small fraction of important gradients to be communicated to the FL server for parameter update during each communication round.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Resource Allocation</strong></p>
<img src="/2023/01/20/FL-Survey/image_g4W43CyUhk.png" class="">
<ul>
<li><p>Participant Selection: less degree of non-iid, computation capabilities, higher loss (fairness allocation: <em>variance</em> of performance of an FL model across participants)</p>
<img src="/2023/01/20/FL-Survey/image_yaGoaxcm3u.png" class=""></li>
<li>Joint Radio and Computation Resource Management: orthogonal access schemes, communication latency increases in direct proportion with the number of participants; for <strong>multiaccess schemes</strong>, latency is independent of the number of participants.</li>
<li>Adaptive Aggragation</li>
<li>Incentive Mechanism</li>
</ul>
</li>
<li><p><strong>Privacy and Security</strong></p>
<ul>
<li><p>Privacy issues</p>
<ul>
<li>Information exploiting attacks in machine learning:  inferring information from a trained model</li>
<li>Differential privacy-based protection solution</li>
<li><p>Collaborative training solutions:</p>
<ul>
<li><p>select gradients/parameters to update</p>
<img src="/2023/01/20/FL-Survey/image_i_UTTnvE8w.png" class=""></li>
<li><p>powerful attack, based on GANs, which allows a <strong>malicious participant</strong> to infer sensitive information from a <strong>victim participant</strong> even with just a part of <strong>shared parameters</strong> from the victim. ← secret sharing scheme (rely on a trusted third party to generate signature key pairs)</p>
<img src="/2023/01/20/FL-Survey/image_NVRS7KdmeO.png" class=""></li>
<li>Collaborative training model: cooperate to train a federated GANs model. (fed GANs can generate artificial data replacing real data for the honest participants). Federated generative model <strong>output artificial data</strong> not belong to any real participants, but comes from the common cross-user data distribution. → training instability &amp; lower performance</li>
</ul>
</li>
<li>Encryption-based Solutions</li>
</ul>
</li>
<li><p>Security issues</p>
<img src="/2023/01/20/FL-Survey/image_mQn5SLRTBs.png" class="">
<ul>
<li>Data Poisoning Attacks: <strong>Dirty-label data</strong> poisoning attacks, a sybil-based data poisoning attack by creating multiple malicious participants</li>
<li>Model Poisoning Attacks: poison the global model that it sends to the server for aggregation. ← check whether the shared model can help to improve the global model’s performance or not. &amp; an updated model from a participant is <strong>too different</strong> from the others</li>
<li><p>Free-Riding Attacks: a participant wants to benefit from the global model without contributing to the learning process. ← Blockchain based, rewards</p>
<img src="/2023/01/20/FL-Survey/image_LD35zj88bP.png" class=""></li>
</ul>
</li>
</ul>
</li>
<li><p>FL for MEN (applications of FL)</p>
<img src="/2023/01/20/FL-Survey/image_SaD4vOtyW5.png" class="">
<h2 id="Federated-Learning-for-Wireless-Communications-Motivation-Opportunities-and-Challenges-Paper-✔️"><a href="#Federated-Learning-for-Wireless-Communications-Motivation-Opportunities-and-Challenges-Paper-✔️" class="headerlink" title="Federated Learning for Wireless Communications: Motivation, Opportunities and Challenges [Paper] ✔️"></a>Federated Learning for Wireless Communications: Motivation, Opportunities and Challenges <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.06847" title="\[Paper]">[Paper]</a> ✔️</h2></li>
</ul>
<img src="/2023/01/20/FL-Survey/image_t2GDn4qdLg.png" class="">
<p>Applications of FL for 5G [difficult to understand…😢]</p>
<ul>
<li>Edge Computing and Caching</li>
<li>Spectrum Management</li>
<li><p>5G Core Network</p>
<h2 id="A-Review-of-Applications-in-Federated-Learning-Paper-✔️"><a href="#A-Review-of-Applications-in-Federated-Learning-Paper-✔️" class="headerlink" title="A Review of Applications in Federated Learning [Paper] ✔️"></a>A Review of Applications in Federated Learning <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0360835220305532" title="\[Paper]">[Paper]</a> ✔️</h2><img src="/2023/01/20/FL-Survey/image_6XTzYhzv48.png" class="">
<img src="/2023/01/20/FL-Survey/image_c6s1iSM5Tp.png" class="">
<img src="/2023/01/20/FL-Survey/image_bhdT4NxPA0.png" class="">
<h2 id="Towards-Efficient-Synchronous-Federated-Training-A-Survey-on-System-Optimization-Strategies-Paper-✔️"><a href="#Towards-Efficient-Synchronous-Federated-Training-A-Survey-on-System-Optimization-Strategies-Paper-✔️" class="headerlink" title="Towards Efficient Synchronous Federated Training: A Survey on System Optimization Strategies [Paper] ✔️"></a>Towards Efficient Synchronous Federated Training: A Survey on System Optimization Strategies <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9780218" title="\[Paper]">[Paper]</a> ✔️</h2></li>
</ul>
<p><strong><em>Time-to-accuracy</em></strong>, which is the wall clock time taken to train a model until it reaches the target accuracy [evaluation metric for efficiency]</p>
<p><strong><em>Challenges</em></strong></p>
<ol>
<li><strong>the lack of information for optimization:</strong> the information needed for optimally configuring the system is usually outdated or simply unavailable due to privacy constraints and scaling issues;</li>
<li><strong>the tradeoff between statistical and system utility:</strong> statistical utility (the number of iterations taken to reach a plausible target accuracy) and system utility (the duration of an iteration), the two determining factor for <em>time-to-accuracy</em>, are usually at odds in FL;&#x20;<ol>
<li>Compare <strong><em>baseline</em></strong>, <strong><em>non-iid</em></strong> [latent Dirichlet allocation], <strong><em>straggler</em></strong> [computation speeds follow the Zipf’s distribution]: \<statistical utility> <strong><em>baseline vs. non-iid</em></strong>; \<system utility > <strong><em>baseline vs. straggler</em></strong></li>
</ol>
</li>
<li><strong>client heterogeneity</strong>: clients cannot be treated uniformly due to the intrinsic differences in terms of resources, data, and states;&#x20;<ol>
<li>Resource Heterogeneity: different capabilities in computation (CPU/GPU/NPU memory, storage) &amp; communication (connectivity, bandwidth) &amp; power (battery level, lifespan) —&gt; system utility</li>
<li>Data Heterogeneity: nonIID (sample quantity &amp; label partition {distribution of data labels}) —&gt; statistical utility</li>
<li>State Heterogeneity: temporal distribution (screen locking, battery charging) —&gt; statistical utility</li>
</ol>
</li>
<li><strong>a large configuration space</strong>: the operational dimensions for system developers are too many to explore within an acceptable time.&#x20;<br><strong><em>Optimizing the time-to-accuracy performance in FL: a layered approach</em></strong> that categorizes them by the training phases in which they take effect:&#x20;<img src="/2023/01/20/FL-Survey/image_lAlPoDDxyP.png" class="">
</li>
</ol>
<img src="/2023/01/20/FL-Survey/image_KyYB5vNNZi.png" class="">
<ol>
<li><strong>Selection</strong>: the server chooses clients for participation, there are mainly two lines of optimization efforts:&#x20;<ol>
<li><strong>prioritizing</strong> clients either with high statistical utility or system utility<ol>
<li>statistical utility oriented: clients with lower degree of non-iid [normalized Euclidean distance (<strong>CSFedAvg</strong>), Reinforced Learning (<strong>FAVOR</strong>)]</li>
<li>system utility oriented: In synchronous training, clients with the lowest system utility bottleneck the speed of a federation round. Bound the time usage: set a deadline for randomly selected clients’ to report updates and ignoring any update submitted after the deadline. To avoid waste of computing resources, <strong>FedCS</strong> takes a step further by proactively selecting a set of clients whose participation is not likely to <em>miss the deadline</em> according to latency estimation results.</li>
</ol>
</li>
<li>explicitly considering both utilities and developing a more informed solution in response to client dynamics in practice.<ol>
<li>Coarse-Grained. TiFL divides clients into <strong>different tiers</strong> based on the <em>observed runtime performance</em>, and at each round only selects clients from the same tier for mitigating the waste of resources due to idle waiting for stragglers. To reduce the average iteration span, it also limits the number of times a (slow) tier can be selected. On top of that, the <em>statistical utility</em> is respected by prioritizing tiers with lower testing accuracy whenever there is more than one electable tier.</li>
<li>Fine-Grained. Oort associates each client with a continuous <strong>score</strong> and prioritizes those clients with higher scores. The score is meant to be a principled measurement of both the statistical utility (determined by the training loss) and the system utility (estimated from historical response latency).&#x20;</li>
</ol>
</li>
</ol>
</li>
<li><strong>Configuration</strong>: the server sends the global model to the selected clients with auxiliary configuration information (e.g., the number of local epochs or the reporting deadline), and clients perform local training, four lines of work:&#x20;<ol>
<li>&#x20;The first two lines advocate mitigating the communication cost by <em>reducing the model size</em> and decreasing the synchronization frequency <ol>
<li><strong>Reducing the model size</strong><ol>
<li><strong>Quantization</strong>. Quantization converts each scalar in a model update to its low-bit representation which takes up less space. <strong>[error feedback]</strong>: tackling the precision loss brought by quantization, the basic idea is to accumulate the previous quantization errors and compensate for them in the current round.</li>
<li><strong>Sketching</strong>. Existing quantization approaches assume the input values follow a certain distribution (e.g., uniform or bell-shaped), which may not always be the case in model updates. To be more general, some researchers introduce sketching methods where some memory-saving data structures are used to approximate the exact distribution of model update values in a single processing pass over the values.&#x20;</li>
<li><strong>Sparsification</strong>. allows each client to transmit only a sparse subset of its model updates, while the rest are accumulated and incorporated into future training. (compatible with cryptographic techniques?)</li>
</ol>
</li>
<li>Decreasing the synchronization frequency <ol>
<li>Client-Level.</li>
<li>Layer-Level. TWAFL DNN update shallow layers more frequently than deep ones as they are more responsible for the overall quality of the global model.</li>
<li>Parameter-Level. consider whether to synchronize for each round at the level of individual parameters. Noticing that each parameter usually evolves <em>in a transient-then-stable manner</em>, i.e., it first varies drastically and then settles down around a certain value with slight oscillation, APF proposes to stop synchronizing those parameters whose evolution has reached a stationary phase.</li>
</ol>
</li>
</ol>
</li>
<li>the last two lines minimize the computational overhead by <em>accelerating the training speed</em> in each round and reducing the number of training rounds<ol>
<li>Accelerating the training speed<ol>
<li>Load Balancing: consider system heterogeneous —&gt; balance the <em>amount</em> of training data across clients &amp; data heterogeneity —&gt; slow client with <em>important training data</em>; Computational Load: varying the number of optimization steps (FedProx) or complexity of local models (HeteroFL: clients with fewer capabilities train smaller sub-models); Communication load: maximize learning efficiency by optimal data batch size and uplink/downlink frame time slots.</li>
</ol>
</li>
<li>Reducing the number of training rounds (convergence speedup)<ol>
<li>Optimizer State Synchronization. If the clients’ momenta are <em>separately</em> updated, they may <em>deviate</em> from each other due to non-IID —&gt; clients’ optimizer states are synchronized by the server periodically.</li>
<li>Client Bias Reduction. Data heterogeneity —&gt; “client drift”: clients’ model updates can be biased towards the minima of local objectives —&gt; hinders the convergence of the global model. To reduce the variance across clients (1) regularize local objective functions for minimizing the drift (2) control variates borrowed from the convex optimization literature (SCAFFOLD, persistent client states); (3) Posterior averaging (FedPA, stateless clients) formulates the optimization as a posterior inference. Compared to traditional federated optimization, posterior inference can benefit from an increased amount of local computation without risking stagnating at inferior optima.&#x20;</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><strong>Reporting</strong>: (aggregation)  two related optimizations:&#x20;<ol>
<li>reducing the aggregation latency by adopting <em>hierarchical methods</em> and developing <em>lightweight</em> privacy preserving methods&#x20;<ol>
<li>Hierarchical methods: changing the aggregation rules, the hierarchical designs focus on establishing the convergence to a single global model.  Clustered FL where clients are assigned to different groups and aggregation takes place within a group —&gt; personalized models for each group of clients.</li>
<li><strong>Lightweight privacy preserving methods</strong> model aggregation with cryptographic techniques —&gt; extra computation and communication overhead. (secure multiparty computation (SMPC)., Homomorphic Encryption)</li>
</ol>
</li>
<li>improving the long-term convergence rate with <strong>adaptive optimizers</strong> on the <strong>server.</strong>&#x20;<ol>
<li>In FL, originally no optimizer at the server —&gt; generalize the existing aggregation algorithm. At each round, instead of collecting local model weights, the server instead collects their changes and treats these changes as the “pseudo-gradient” for the <strong>server</strong>, which the server can use to update the global model with <strong>adaptive optimizers</strong>.<img src="/2023/01/20/FL-Survey/image_HU9QW1oSPU.png" class="">
<strong>Training Datasets</strong>:&#x20;</li>
</ol>
</li>
</ol>
</li>
</ol>
<p>(1) Synthetic: derived from conventional ML datasets (e.g.,CIFAR, MNIST, and Fashion-MNIST). To <em>synthesize</em> the non-IID nature as in real FL scenarios, the data partitions in these datasets are typically formed by restricting the number of data classes each client has (e.g., partitioning by shard-based methods or latent Dirichlet allocation (LDA))</p>
<p>(2) Realistic: LEAF, FedScale, OARF</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Jiaqi Shao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://luuvy757.github.io/2023/01/20/FL-Survey/">https://luuvy757.github.io/2023/01/20/FL-Survey/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/literature/">literature</a></div><div class="post_share"><div class="social-share" data-image="/img/covers/20220310232325.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/09/15/technical-writing/"><img class="next-cover" src="/img/covers/20220310232325.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">technical writing</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Jiaqi Shao</div><div class="author-info__description">description</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">7</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">2</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/luuvy757"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Survey"><span class="toc-number">1.</span> <span class="toc-text">Survey</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Federated-Machine-Learning-Concept-and-Applications-Paper-%E2%9C%94%EF%B8%8F"><span class="toc-number">1.1.</span> <span class="toc-text">Federated Machine Learning: Concept and Applications [Paper] ✔️</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Federated-Learning-Challenges-Methods-and-Future-Directions-Paper-%E2%9C%94%EF%B8%8F"><span class="toc-number">1.2.</span> <span class="toc-text">Federated Learning: Challenges, Methods, and Future Directions [Paper] ✔️</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Advances-and-Open-Problems-in-Federated-Learning-Paper"><span class="toc-number">1.3.</span> <span class="toc-text">Advances and Open Problems in Federated Learning [Paper]</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Federated-Learning-White-Paper-V1-0-Paper-%E2%9C%94%EF%B8%8F"><span class="toc-number">1.4.</span> <span class="toc-text">Federated Learning White Paper V1.0 [Paper] ✔️</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Federated-Learning-Systems-Vision-Hype-and-Reality-for-Data-Privacy-and-Protection-Paper-%E2%9C%94%EF%B8%8F"><span class="toc-number">1.5.</span> <span class="toc-text">Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection [Paper] ✔️</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Federated-Learning-in-Mobile-Edge-Networks-A-Comprehensive-Survey-Paper-%E2%9C%94%EF%B8%8F"><span class="toc-number">1.6.</span> <span class="toc-text">Federated Learning in Mobile Edge Networks: A Comprehensive Survey [Paper] ✔️</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Federated-Learning-for-Wireless-Communications-Motivation-Opportunities-and-Challenges-Paper-%E2%9C%94%EF%B8%8F"><span class="toc-number">1.7.</span> <span class="toc-text">Federated Learning for Wireless Communications: Motivation, Opportunities and Challenges [Paper] ✔️</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A-Review-of-Applications-in-Federated-Learning-Paper-%E2%9C%94%EF%B8%8F"><span class="toc-number">1.8.</span> <span class="toc-text">A Review of Applications in Federated Learning [Paper] ✔️</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Towards-Efficient-Synchronous-Federated-Training-A-Survey-on-System-Optimization-Strategies-Paper-%E2%9C%94%EF%B8%8F"><span class="toc-number">1.9.</span> <span class="toc-text">Towards Efficient Synchronous Federated Training: A Survey on System Optimization Strategies [Paper] ✔️</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/01/20/FL-Survey/" title="FL Survey"><img src="/img/covers/20220310232325.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="FL Survey"/></a><div class="content"><a class="title" href="/2023/01/20/FL-Survey/" title="FL Survey">FL Survey</a><time datetime="2023-01-20T02:17:09.000Z" title="Created 2023-01-20 10:17:09">2023-01-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/15/technical-writing/" title="technical writing"><img src="/img/covers/20220310232325.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="technical writing"/></a><div class="content"><a class="title" href="/2022/09/15/technical-writing/" title="technical writing">technical writing</a><time datetime="2022-09-15T12:08:58.000Z" title="Created 2022-09-15 20:08:58">2022-09-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/03/28/CSC4005-Parallel-Computing/" title="CSC4005 Parallel Computing"><img src="/img/covers/202203102327.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CSC4005 Parallel Computing"/></a><div class="content"><a class="title" href="/2022/03/28/CSC4005-Parallel-Computing/" title="CSC4005 Parallel Computing">CSC4005 Parallel Computing</a><time datetime="2022-03-28T09:13:55.000Z" title="Created 2022-03-28 17:13:55">2022-03-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/03/25/CSC3150-Operating-System/" title="CSC3150 Operating System"><img src="/img/covers/2022210231764265f3565f96825.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CSC3150 Operating System"/></a><div class="content"><a class="title" href="/2022/03/25/CSC3150-Operating-System/" title="CSC3150 Operating System">CSC3150 Operating System</a><time datetime="2022-03-24T16:24:13.000Z" title="Created 2022-03-25 00:24:13">2022-03-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/03/22/CSC3050-Computer-Architecture/" title="CSC3050 Computer Architecture"><img src="/img/covers/20220310232424.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CSC3050 Computer Architecture"/></a><div class="content"><a class="title" href="/2022/03/22/CSC3050-Computer-Architecture/" title="CSC3050 Computer Architecture">CSC3050 Computer Architecture</a><time datetime="2022-03-22T05:59:55.000Z" title="Created 2022-03-22 13:59:55">2022-03-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Jiaqi Shao</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="https://cdn.jsdelivr.net/npm/live2d-widget@^3.1.3/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"width":200,"height":200,"position":"right","hOffset":30,"vOffset":-20},"mobile":{"show":true},"react":{"opacityDefault":1,"opacityOnHover":0.2},"log":false});</script></body></html>